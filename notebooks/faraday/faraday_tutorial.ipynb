{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faraday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial explains how Faraday works and how to use Faraday to train a generative model and generate synthetic smart meter data.\n",
    "\n",
    "---\n",
    "\n",
    "Faraday is a Conditional Variational Auto-Encoder (VAE)-based model. Unlike traditional VAEs where seeds are drawn from a normal distribution and decoded, Faraday works by:\n",
    "1. First train a VAE using the following loss functions: a. MMD instead of KL-divergence b. Quantile losses and c. Mean squared error\n",
    "2. Encode real samples to the latent space using the encoder, and fit a Gaussian Mixture Model (GMM) over the latent space.\n",
    "3. During inference, draw samples from the GMM and decode with the decoder.\n",
    "\n",
    "For more information on Faraday's architecture, refer to the [Faraday paper](https://arxiv.org/abs/2404.04314).\n",
    "\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "If you haven't already, please download LCL dataset from [data.london.gov.uk](https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíø Loading LCL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from opensynth.data_modules.lcl_data_module import LCLDataModule\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = Path(\"../../data/processed/historical/train/lcl_data.csv\")\n",
    "stats_path = Path(\"../../data/processed/historical/train/mean_std.csv\")\n",
    "outlier_path = Path(\"../../data/processed/historical/train/outliers.csv\")\n",
    "\n",
    "dm = LCLDataModule(data_path=data_path, stats_path=stats_path, batch_size=25000, n_samples=100000)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_with_outliers = LCLDataModule(data_path=data_path, stats_path=stats_path, batch_size=200, n_samples=20000, outlier_path=outlier_path)\n",
    "dm_with_outliers.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ VAE Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensynth.models.faraday.vae_model import FaradayVAE\n",
    "# Option to pass in your own encoder architecture in the future\n",
    "model = FaradayVAE(class_dim=2, latent_dim=16, learning_rate=0.001, mse_weight=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size 500 is when MPS becomes faster than CPU..\n",
    "# But sometimes large batch size hurts convergence..\n",
    "# Suggest training on CPU with small batch size\n",
    "# And potentially experiment with best hyperparameters on large batch size before using 'mps'\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=150, accelerator=\"auto\")\n",
    "trainer.fit(model, dm_with_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model, \"vae_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üï∏Ô∏è GMM Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensynth.models.faraday.model import FaradayModel\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"faraday_model.pt\")\n",
    "model = torch.load(\"vae_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to update model s.t. `feature_list` is saved if want to load model from checkpoint. For now, use trained VAE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FaradayVAE.load_from_checkpoint(\"lightning_logs/version_0/checkpoints/epoch=249-step=25000.ckpt\", map_location=torch.device('cpu'))\n",
    "\n",
    "# faraday_model_1500 = FaradayModel(vae_module=model, n_components=1500, max_iter=100, tol=1e-2)\n",
    "# faraday_model_150 = FaradayModel(vae_module=model, n_components=150, max_iter=100, tol=1e-2)\n",
    "# faraday_model_50 = FaradayModel(vae_module=model, n_components=50, max_iter=100, tol=1e-2)\n",
    "# faraday_model_10 = FaradayModel(vae_module=model, n_components=10, max_iter=100, tol=1e-2)\n",
    "# faraday_model_1 = FaradayModel(vae_module=model, n_components=1, max_iter=100, tol=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "faraday_model_1500 = FaradayModel(\n",
    "    vae_module=model, n_components=1500, tol=1e-2, gmm_max_epochs=100,  gmm_covariance_reg=1e-4,\n",
    ")\n",
    "\n",
    "faraday_model_150 = FaradayModel(\n",
    "    vae_module=model, n_components=150, tol=1e-2, gmm_max_epochs=100, gmm_covariance_reg=1e-4,\n",
    ")\n",
    "\n",
    "faraday_model_50 = FaradayModel(\n",
    "    vae_module=model, n_components=50, tol=1e-2, gmm_max_epochs=100, gmm_covariance_reg=1e-4,\n",
    ")\n",
    "\n",
    "\n",
    "faraday_model_10 = FaradayModel(\n",
    "    vae_module=model, n_components=10, tol=1e-2, gmm_max_epochs=100, gmm_covariance_reg=1e-4,\n",
    ")\n",
    "\n",
    "faraday_model_1 = FaradayModel(\n",
    "    vae_module=model, n_components=1, tol=1e-2, gmm_max_epochs=100, gmm_covariance_reg=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_data_module = LCLDataModule(data_path=data_path, stats_path=stats_path, batch_size=25000, n_samples=100000, outlier_path=outlier_path)\n",
    "gmm_data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                      | Type                    | Params | Mode \n",
      "------------------------------------------------------------------------------\n",
      "0 | gmm_module                | GaussianMixtureModel    | 0      | train\n",
      "1 | vae_module                | FaradayVAE              | 402 K  | train\n",
      "2 | weight_metric             | WeightsMetric           | 0      | train\n",
      "3 | mean_metric               | MeansMetric             | 0      | train\n",
      "4 | precision_cholesky_metric | PrecisionCholeskyMetric | 0      | train\n",
      "5 | covariance_metric         | CovarianceMetric        | 0      | train\n",
      "6 | nll                       | NLLMetric               | 0      | train\n",
      "------------------------------------------------------------------------------\n",
      "402 K     Trainable params\n",
      "0         Non-trainable params\n",
      "402 K     Total params\n",
      "1.609     Total estimated model params size (MB)\n",
      "38        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6399)\n",
      "tensor(1.)\n",
      "Initial prec chol: 0.6398732662200928.                 Initial mean: 1.5001500844955444\n",
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.40it/s, v_num=120]Local weights at rank: 0 - means: 0.0136, 0.7431\n",
      "Reduced weights, means, covar: 0.0136,0.7431, 2.2767\n",
      "NLL:  tensor(5.6046)\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.61it/s, v_num=120]Local weights at rank: 0 - means: 0.0235, 0.8159\n",
      "Reduced weights, means, covar: 0.0235,0.8159, 1.5115\n",
      "NLL:  tensor(4.8463)\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.49it/s, v_num=120]Local weights at rank: 0 - means: 0.0307, 0.8763\n",
      "Reduced weights, means, covar: 0.0307,0.8763, 1.1191\n",
      "NLL:  tensor(4.7586)\n",
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.50it/s, v_num=120]Local weights at rank: 0 - means: 0.0379, 0.8284\n",
      "Reduced weights, means, covar: 0.0379,0.8284, 0.8263\n",
      "NLL:  tensor(4.6384)\n",
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.66it/s, v_num=120]Local weights at rank: 0 - means: 0.0474, 0.7392\n",
      "Reduced weights, means, covar: 0.0474,0.7392, 0.6049\n",
      "NLL:  tensor(4.5728)\n",
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.51it/s, v_num=120]Local weights at rank: 0 - means: 0.0545, 0.6681\n",
      "Reduced weights, means, covar: 0.0545,0.6681, 0.5063\n",
      "NLL:  tensor(4.5169)\n",
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.49it/s, v_num=120]Local weights at rank: 0 - means: 0.0605, 0.6349\n",
      "Reduced weights, means, covar: 0.0605,0.6349, 0.4386\n",
      "NLL:  tensor(4.4855)\n",
      "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.44it/s, v_num=120]Local weights at rank: 0 - means: 0.0680, 0.5873\n",
      "Reduced weights, means, covar: 0.0680,0.5873, 0.4057\n",
      "NLL:  tensor(4.4587)\n",
      "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.48it/s, v_num=120]Local weights at rank: 0 - means: 0.0726, 0.5814\n",
      "Reduced weights, means, covar: 0.0726,0.5814, 0.3725\n",
      "NLL:  tensor(4.4439)\n",
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.25it/s, v_num=120]Local weights at rank: 0 - means: 0.0777, 0.5794\n",
      "Reduced weights, means, covar: 0.0777,0.5794, 0.3568\n",
      "NLL:  tensor(4.4181)\n",
      "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.41it/s, v_num=120]Local weights at rank: 0 - means: 0.0824, 0.5389\n",
      "Reduced weights, means, covar: 0.0824,0.5389, 0.3155\n",
      "NLL:  tensor(4.4156)\n",
      "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.41it/s, v_num=120]\n"
     ]
    }
   ],
   "source": [
    "faraday_model_10.train_gmm(dm=gmm_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                      | Type                    | Params | Mode \n",
      "------------------------------------------------------------------------------\n",
      "0 | gmm_module                | GaussianMixtureModel    | 0      | train\n",
      "1 | vae_module                | FaradayVAE              | 402 K  | train\n",
      "2 | weight_metric             | WeightsMetric           | 0      | train\n",
      "3 | mean_metric               | MeansMetric             | 0      | train\n",
      "4 | precision_cholesky_metric | PrecisionCholeskyMetric | 0      | train\n",
      "5 | nll                       | NLLMetric               | 0      | train\n",
      "------------------------------------------------------------------------------\n",
      "402 K     Trainable params\n",
      "0         Non-trainable params\n",
      "402 K     Total params\n",
      "1.609     Total estimated model params size (MB)\n",
      "37        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prec chol: 1.5353962182998657.                 Initial mean: 0.4759025275707245\n",
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.09it/s, v_num=129]Local weights at rank: 0 - means: 0.0091, 0.3047\n",
      "Reduced weights, means: 0.0091,0.3047, \n",
      "NLL:  tensor(4.1499)\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.09it/s, v_num=129]Local weights at rank: 0 - means: 0.0134, 0.1913\n",
      "Reduced weights, means: 0.0134,0.1913, \n",
      "NLL:  tensor(3.6442)\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.90it/s, v_num=129]Local weights at rank: 0 - means: 0.0162, 0.1454\n",
      "Reduced weights, means: 0.0162,0.1454, \n",
      "NLL:  tensor(3.4114)\n",
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.88it/s, v_num=129]Local weights at rank: 0 - means: 0.0187, 0.0777\n",
      "Reduced weights, means: 0.0187,0.0777, \n",
      "NLL:  tensor(3.3159)\n",
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.05it/s, v_num=129]Local weights at rank: 0 - means: 0.0211, 0.0088\n",
      "Reduced weights, means: 0.0211,0.0088, \n",
      "NLL:  tensor(3.1954)\n",
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.96it/s, v_num=129]Local weights at rank: 0 - means: 0.0240, -0.0140\n",
      "Reduced weights, means: 0.0240,-0.0140, \n",
      "NLL:  tensor(3.1436)\n",
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.87it/s, v_num=129]Local weights at rank: 0 - means: 0.0255, -0.0429\n",
      "Reduced weights, means: 0.0255,-0.0429, \n",
      "NLL:  tensor(3.1580)\n",
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.87it/s, v_num=129]\n"
     ]
    }
   ],
   "source": [
    "faraday_model_50.train_gmm(dm=gmm_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                      | Type                    | Params | Mode \n",
      "------------------------------------------------------------------------------\n",
      "0 | gmm_module                | GaussianMixtureModel    | 0      | train\n",
      "1 | vae_module                | FaradayVAE              | 402 K  | train\n",
      "2 | weight_metric             | WeightsMetric           | 0      | train\n",
      "3 | mean_metric               | MeansMetric             | 0      | train\n",
      "4 | precision_cholesky_metric | PrecisionCholeskyMetric | 0      | train\n",
      "5 | covariance_metric         | CovarianceMetric        | 0      | train\n",
      "6 | nll                       | NLLMetric               | 0      | train\n",
      "------------------------------------------------------------------------------\n",
      "402 K     Trainable params\n",
      "0         Non-trainable params\n",
      "402 K     Total params\n",
      "1.609     Total estimated model params size (MB)\n",
      "38        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2308)\n",
      "tensor(1.0000)\n",
      "Initial prec chol: 2.2307753562927246.                 Initial mean: 1.3672646284103394\n",
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.14it/s, v_num=122]Local weights at rank: 0 - means: 0.0017, 1.0868\n",
      "Reduced weights, means, covar: 0.0017,1.0868, 0.1874\n",
      "NLL:  tensor(3.4614)\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.43it/s, v_num=122]Local weights at rank: 0 - means: 0.0018, 0.8566\n",
      "Reduced weights, means, covar: 0.0018,0.8566, 0.1473\n",
      "NLL:  tensor(2.9998)\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.32it/s, v_num=122]Local weights at rank: 0 - means: 0.0023, 0.8240\n",
      "Reduced weights, means, covar: 0.0023,0.8240, 0.0956\n",
      "NLL:  tensor(2.7432)\n",
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.33it/s, v_num=122]Local weights at rank: 0 - means: 0.0030, 0.7130\n",
      "Reduced weights, means, covar: 0.0030,0.7130, 0.1017\n",
      "NLL:  tensor(2.6538)\n",
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.34it/s, v_num=122]Local weights at rank: 0 - means: 0.0049, 0.7192\n",
      "Reduced weights, means, covar: 0.0049,0.7192, 0.1310\n",
      "NLL:  tensor(2.5690)\n",
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.22it/s, v_num=122]Local weights at rank: 0 - means: 0.0069, 0.6296\n",
      "Reduced weights, means, covar: 0.0069,0.6296, 0.1217\n",
      "NLL:  tensor(2.5938)\n",
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.22it/s, v_num=122]\n"
     ]
    }
   ],
   "source": [
    "faraday_model_150.train_gmm(dm=gmm_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ligthning_sum_components = faraday_model_150.gmm_module.means.sum(axis=1)\n",
    "len(ligthning_sum_components[ligthning_sum_components==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/4 [14:50<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                      | Type                    | Params | Mode \n",
      "------------------------------------------------------------------------------\n",
      "0 | gmm_module                | GaussianMixtureModel    | 0      | train\n",
      "1 | vae_module                | FaradayVAE              | 402 K  | train\n",
      "2 | weight_metric             | WeightsMetric           | 0      | train\n",
      "3 | mean_metric               | MeansMetric             | 0      | train\n",
      "4 | precision_cholesky_metric | PrecisionCholeskyMetric | 0      | train\n",
      "5 | nll                       | NLLMetric               | 0      | train\n",
      "------------------------------------------------------------------------------\n",
      "402 K     Trainable params\n",
      "0         Non-trainable params\n",
      "402 K     Total params\n",
      "1.609     Total estimated model params size (MB)\n",
      "37        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prec chol: 5.030717849731445.                 Initial mean: 1.7637825012207031\n",
      "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:16<00:00,  0.24it/s, v_num=131]\n"
     ]
    }
   ],
   "source": [
    "faraday_model_1500.train_gmm(dm=gmm_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(faraday_model_1500, \"faraday_model_1500.pt\")\n",
    "# torch.save(faraday_model_150, \"faraday_model_150.pt\")\n",
    "# torch.save(faraday_model_50, \"faraday_model_50.pt\")\n",
    "# torch.save(faraday_model_10, \"faraday_model_10.pt\")\n",
    "# torch.save(faraday_model_1, \"faraday_model_1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Comparing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Generating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_samples(model, n_samples, dm):\n",
    "    gmm_samples = model.sample_gmm(n_samples)\n",
    "    gmm_samples_reconstructed = dm.reconstruct_kwh(gmm_samples['kwh'])\n",
    "    gmm_samples_reconstructed = torch.clip(gmm_samples_reconstructed, min=0)\n",
    "    return gmm_samples_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GaussianMixtureModel' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gmm_1500 \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_synthetic_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfaraday_model_1500\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m gmm_150 \u001b[38;5;241m=\u001b[39m generate_synthetic_samples(faraday_model_150, \u001b[38;5;241m20000\u001b[39m, dm)\n\u001b[1;32m      3\u001b[0m gmm_50 \u001b[38;5;241m=\u001b[39m generate_synthetic_samples(faraday_model_50, \u001b[38;5;241m20000\u001b[39m, dm)\n",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m, in \u001b[0;36mgenerate_synthetic_samples\u001b[0;34m(model, n_samples, dm)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_synthetic_samples\u001b[39m(model, n_samples, dm):\n\u001b[0;32m----> 2\u001b[0m     gmm_samples \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_gmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     gmm_samples_reconstructed \u001b[38;5;241m=\u001b[39m dm\u001b[38;5;241m.\u001b[39mreconstruct_kwh(gmm_samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkwh\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m     gmm_samples_reconstructed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclip(gmm_samples_reconstructed, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/OpenSynth/src/opensynth/models/faraday/model.py:302\u001b[0m, in \u001b[0;36mFaradayModel.sample_gmm\u001b[0;34m(self, n_samples)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_gmm\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_samples: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TrainingData:\n\u001b[1;32m    292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    Samples latent codes from GMM and decode with decoder.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m        TrainingData: Decoder output (KWH), feature labels\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     gmm_samples: np\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgmm_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msample(n_samples)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# Parse GMM samples\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     gmm_samples_parsed: TrainingData \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_samples(\n\u001b[1;32m    307\u001b[0m         gmm_samples,\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae_module\u001b[38;5;241m.\u001b[39mlatent_dim,\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae_module\u001b[38;5;241m.\u001b[39mfeature_list,\n\u001b[1;32m    310\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GaussianMixtureModel' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "gmm_1500 = generate_synthetic_samples(faraday_model_1500, 20000, dm)\n",
    "gmm_150 = generate_synthetic_samples(faraday_model_150, 20000, dm)\n",
    "gmm_50 = generate_synthetic_samples(faraday_model_50, 20000, dm)\n",
    "gmm_10 = generate_synthetic_samples(faraday_model_10, n_samples=20000, dm=dm)\n",
    "gmm_1 = generate_synthetic_samples(faraday_model_1, 20000, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_kwh = dm.reconstruct_kwh(next(iter(gmm_data_module.train_dataloader()))['kwh'])\n",
    "real_kwh = torch.clip(real_kwh, min=0) # Clip min 0 to get read of negative values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Comparing mean, 95th quantile, median profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(real_kwh, gmm_reconstruct):\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 3), sharey=True, gridspec_kw={'wspace': 0.05})\n",
    "\n",
    "    ax1.plot(real_kwh.mean(dim=0).detach().numpy(), label=\"real kwh\")\n",
    "    ax1.plot(gmm_reconstruct.mean(dim=0).detach().numpy(), label=\"gmm kwh\")\n",
    "    ax1.set_title(\"Mean kWh per half hour\")\n",
    "    ax1.set_xlabel(\"Settlement Periods\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(real_kwh.quantile(0.95, dim=0).detach().numpy(), label=\"real kwh\")\n",
    "    ax2.plot(gmm_reconstruct.quantile(0.95, dim=0).detach().numpy(), label=\"gmm kwh\")\n",
    "    ax2.set_title(\"95th Quantile kWh per half hour\")\n",
    "    ax2.set_xlabel(\"Settlement Periods\")\n",
    "    ax2.legend()\n",
    "\n",
    "    ax3.plot(real_kwh.quantile(0.5, dim=0).detach().numpy(), label=\"real kwh\")\n",
    "    ax3.set_title(\"Median kWh per half hour\")\n",
    "    ax3.set_xlabel(\"Settlement Periods\")\n",
    "    ax3.plot(gmm_reconstruct.quantile(0.5, dim=0).detach().numpy(), label=\"gmm kwh\")\n",
    "    ax3.legend()\n",
    "\n",
    "    fig.text(0.1, 0.5, 'kWh', va='center', rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(real_kwh, gmm_1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(real_kwh, gmm_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(real_kwh, gmm_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(real_kwh, gmm_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(real_kwh, gmm_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PCA and TSNE Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_pca_and_tsne(real_kwh, gmm_reconstruct):\n",
    "    pca = PCA(n_components=2)\n",
    "    tsne = TSNE(n_components=2)\n",
    "\n",
    "    pca.fit(real_kwh.detach().numpy())\n",
    "    pca_real = pca.transform(real_kwh.detach().numpy())\n",
    "    pca_gmm = pca.transform(gmm_reconstruct.detach().numpy())\n",
    "\n",
    "    tsne_input = np.concatenate([real_kwh.detach().numpy(), gmm_reconstruct.detach().numpy()])\n",
    "    tsne_results = tsne.fit_transform(tsne_input)\n",
    "    tsne_real = tsne_results[:len(real_kwh)]\n",
    "    tsne_gmm = tsne_results[len(real_kwh):]\n",
    "\n",
    "    return pca_real, pca_gmm, tsne_real, tsne_gmm\n",
    "\n",
    "\n",
    "def plot_pca_tsne(pca_real, pca_gmm, tsne_real, tsne_gmm):\n",
    "    fig, (ax_pca, ax_tsne) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    ax_pca.scatter(pca_real[:, 0], pca_real[:, 1], label=\"real kwh\", s=0.3, alpha=0.5)\n",
    "    ax_pca.scatter(pca_gmm[:, 0], pca_gmm[:, 1], label=\"gmm kwh\", s=0.3, alpha=0.5)\n",
    "    ax_pca.set_title(\"PCA\")\n",
    "    ax_pca.set_xlabel(\"PCA 1\")\n",
    "    ax_pca.set_ylabel(\"PCA 2\")\n",
    "    ax_pca.legend()\n",
    "\n",
    "    ax_tsne.scatter(tsne_real[:, 0], tsne_real[:, 1], label=\"real kwh\", s=0.3, alpha=0.5)\n",
    "    ax_tsne.scatter(tsne_gmm[:, 0], tsne_gmm[:, 1], label=\"gmm kwh\", s=0.3, alpha=0.5)\n",
    "    ax_tsne.set_title(\"TSNE\")\n",
    "    ax_tsne.set_xlabel(\"TSNE 1\")\n",
    "    ax_tsne.set_ylabel(\"TSNE 2\")\n",
    "    ax_tsne.legend()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, a2, a3, a4 = train_pca_and_tsne(real_kwh, gmm_1500)\n",
    "_ = plot_pca_tsne(a1, a2, a3, a4)\n",
    "plt.title(\"GMM with 1500 clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1, b2, b3, b4 = train_pca_and_tsne(real_kwh, gmm_150)\n",
    "_ = plot_pca_tsne(b1, b2, b3, b4)\n",
    "plt.title(\"GMM with 150 clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2, c3, c4 = train_pca_and_tsne(real_kwh, gmm_50)\n",
    "_ = plot_pca_tsne(c1, c2, c3, c4)\n",
    "plt.title(\"GMM with 50 clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d2, d3, d4 = train_pca_and_tsne(real_kwh, gmm_10)\n",
    "_ = plot_pca_tsne(d1, d2, d3, d4)\n",
    "plt.title(\"GMM with 10 clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1, e2, e3, e4 = train_pca_and_tsne(real_kwh, gmm_1)\n",
    "_ = plot_pca_tsne(e1, e2, e3, e4)\n",
    "plt.title(\"GMM with 1 clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõÉ Customising your VAE Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to also customise your VAE archicture without touching the rest of Faraday code. You can do this by:\n",
    "\n",
    "1. Creating a custom class inheriting from the Encoder and Decoder module\n",
    "2. Using `super().__init__()` to inherit all the attributes and methods of the parent class\n",
    "3. Overriding the attribute `encoder_layers` or `decoder_layers`. \n",
    "\n",
    "In this example, we'll be showing how to do this with simple linear layers, but in reality you could use more complicated architectures such as Conv1D layers, or LSTM layers.\n",
    "For more complex layers, you'll need to make sure that you've shaped the inputs correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensynth.models.faraday.vae_model import FaradayVAE, Encoder, Decoder\n",
    "from opensynth.models.faraday.model import FaradayModel\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Encoder and Decoder Architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEncoderModule(Encoder):\n",
    "    \"\"\"\n",
    "    Custom Encoder Module\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int, input_dim: int, class_dim: int):\n",
    "        \"\"\"\n",
    "        Inherit parent encoder attributes and methods.\n",
    "        But we will be overriding the `encoder_layers` attribute\n",
    "        with our custom encoder architecture.\n",
    "\n",
    "        When inheriting from parent `Encoder` class, we need to\n",
    "        pass in the attributes: latent_dim, input_dim, class_dim.\n",
    "\n",
    "        Outputs of encoder_layers should be `latent_dim`.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Latent dimension.\n",
    "            input_dim (int): Input dimensions.\n",
    "            class_dim (int): Class dimensions.\n",
    "        \"\"\"\n",
    "        super().__init__(latent_dim=latent_dim, input_dim=input_dim, class_dim=class_dim)\n",
    "        self.encoder_layers = nn.Sequential(\n",
    "            nn.Linear(self.encoder_input_dim, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, self.latent_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "class CustomDecoderModule(Decoder):\n",
    "    \"\"\"\n",
    "    Custom Decoder Module\n",
    "    \"\"\"\n",
    "    def __init__(self, class_dim: int, latent_dim: int, output_dim: int):\n",
    "        \"\"\"\n",
    "        Inherit parent decoder attributes and methods.\n",
    "        But we will be overriding the `decoder_layers` attribute\n",
    "        with our custom decoder architecture.\n",
    "\n",
    "        When inheriting from parent `Decoder` class, we need to\n",
    "        pass in the attributes: class_dim, latent_dim, output_dim.\n",
    "\n",
    "        Outputs of encoder_layers should be `output_dim`.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Latent dimension.\n",
    "            output_dim (int): Output dimensions.\n",
    "            class_dim (int): Class dimensions.\n",
    "        \"\"\"\n",
    "        super().__init__(class_dim=class_dim, latent_dim=latent_dim, output_dim=output_dim)\n",
    "        self.decoder_layers = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, self.output_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Faraday VAE with custom encoder and decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_encoder = CustomEncoderModule(class_dim=2, latent_dim=16, input_dim=48)\n",
    "custom_decoder = CustomDecoderModule(class_dim=2, latent_dim=16, output_dim=48)\n",
    "\n",
    "faraday_custom_vae = FaradayVAE(\n",
    "    class_dim=2, \n",
    "    latent_dim=16, \n",
    "    learning_rate=0.001, \n",
    "    mse_weight=3, \n",
    "    custom_encoder=custom_encoder, \n",
    "    custom_decoder=custom_decoder\n",
    ")\n",
    "\n",
    "custom_trainer = pl.Trainer(max_epochs=250, accelerator=\"cpu\")\n",
    "custom_trainer.fit(faraday_custom_vae, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faraday_custom_model = FaradayModel(vae_module=faraday_custom_vae, n_components=15, tol=1e-5, gmm_covariance_reg=1e-3)\n",
    "gmm_data_module = LCLDataModule(data_path=data_path, stats_path=stats_path, batch_size=5000, n_samples=50000)\n",
    "gmm_data_module.setup()\n",
    "faraday_custom_model.train_gmm(dm=gmm_data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_gmm_samples = faraday_custom_model.sample_gmm(n_samples=10000)\n",
    "\n",
    "custom_gmm_kwh = custom_gmm_samples['kwh']\n",
    "custom_gmm_reconstruct = dm.reconstruct_kwh(custom_gmm_kwh)\n",
    "custom_gmm_reconstruct = torch.clip(custom_gmm_reconstruct, min=0) # Clip min 0 to get read of negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_real_kwh = dm.reconstruct_kwh(next(iter(gmm_data_module.train_dataloader()))['kwh'])\n",
    "custom_real_kwh = torch.clip(custom_real_kwh, min=0) # Clip min 0 to get read of negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(custom_real_kwh, custom_gmm_reconstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = train_pca_and_tsne(custom_real_kwh, custom_gmm_reconstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_pca_tsne(a, b, c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîê Training with Differential Privacy (DP-SGD)\n",
    "\n",
    "Privacy should be one of the concerns with genarating synthetic data, especially when done for data sharing purposes. To implement privacy, we use Differentially-Private Stochastic Gradient Descent [[1]](https://arxiv.org/abs/1607.00133) implemented with Pytorch Opacus library [[2]](https://opacus.ai/).\n",
    "\n",
    "You can train Faraday with Differential Privacy turned on with the `differential_privacy` parameter as demonstrated below. When implementing DP-SGD, you need to specify the following:\n",
    "\n",
    "1. `epsilon` - the level of privacy (high epsilon = less private)\n",
    "2. `delta` - this should be $<<1/N$, where $N$ is the size of the dataset\n",
    "\n",
    "For more context and considerations between Privacy and synthetic smart meter generation, e.g. on deciding `epsilon` value, check out this paper from Centre for Net Zero: [Defining \"Good\": Evalution Framework for SYnthetic Smart Meter Data](https://arxiv.org/abs/2407.11785)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensynth.models.faraday.vae_model import FaradayVAE\n",
    "\n",
    "vae_dp = FaradayVAE(\n",
    "    class_dim=2,\n",
    "    latent_dim=16,\n",
    "    learning_rate=0.001,\n",
    "    mse_weight=3,\n",
    "    differential_privacy=True,\n",
    "    epsilon=8.0,\n",
    "    delta=1/20000,\n",
    ")\n",
    "\n",
    "dp_trainer = pl.Trainer(max_epochs=25, accelerator=\"cpu\")\n",
    "dp_trainer.fit(vae_dp, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenSynth-BNsxhSIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
