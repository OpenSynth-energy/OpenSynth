{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "full_data = pd.read_csv(\"../../data/processed/historical/train/lcl_data.csv\")\n",
    "df_100K = full_data.sample(100000, random_state=0)\n",
    "df_100K.to_csv(\"../../data/processed/historical/train/lcl_data_100K.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "RANDOM_STATE = 0\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(RANDOM_STATE)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from opensynth.data_modules.lcl_data_module import LCLDataModule\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = Path(\"../../data/processed/historical/train/lcl_data_100K.csv\")\n",
    "stats_path = Path(\"../../data/processed/historical/train/mean_std.csv\")\n",
    "outlier_path = Path(\"../../data/processed/historical/train/outliers.csv\")\n",
    "\n",
    "dm = LCLDataModule(data_path=data_path, stats_path=stats_path, batch_size=100000, n_samples=100000)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FaradayVAE(\n",
       "  (encoder): Encoder(\n",
       "    (encoder_layers): Sequential(\n",
       "      (0): Linear(in_features=50, out_features=512, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "      (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (7): GELU(approximate='none')\n",
       "      (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (9): GELU(approximate='none')\n",
       "      (10): Linear(in_features=32, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (latent): Linear(in_features=18, out_features=16, bias=True)\n",
       "    (latent_activations): GELU(approximate='none')\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "      (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (7): GELU(approximate='none')\n",
       "      (8): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (9): GELU(approximate='none')\n",
       "      (10): Linear(in_features=512, out_features=48, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (reparametriser): ReparametrisationModule(\n",
       "    (mean): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (logvar): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from opensynth.models.faraday import FaradayVAE\n",
    "vae_model = torch.load(\"vae_model.pt\")\n",
    "vae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensynth.models.faraday.gaussian_mixture.prepare_gmm_input import encode_data_for_gmm\n",
    "\n",
    "next_batch = next(iter(dm.train_dataloader()))\n",
    "input_tensor = encode_data_for_gmm(data=next_batch, vae_module=vae_model)\n",
    "input_data = input_tensor.detach().numpy()\n",
    "n_samples = len(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COMPONENTS = 700\n",
    "REG_COVAR = 1e-4\n",
    "EPOCHS = 25\n",
    "IDX = 0\n",
    "CONVERGENCE_TOL = 1e-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100000, 18]), tensor(0.4973, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, input_tensor[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "from opensynth.models.faraday.new_gmm import gmm_utils\n",
    "\n",
    "labels_, means_, responsibilities_ = gmm_utils.initialise_centroids(\n",
    "        X=input_data, n_components=N_COMPONENTS\n",
    "    )\n",
    "print(labels_.dtype, responsibilities_.dtype, means_.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.2479)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "from opensynth.models.faraday.new_gmm.train_gmm import initialise_gmm_params\n",
    "\n",
    "gmm_init_params = initialise_gmm_params(\n",
    "    X=input_data,\n",
    "    n_components = N_COMPONENTS,\n",
    "    reg_covar=REG_COVAR,\n",
    ")\n",
    "print(gmm_init_params[\"precision_cholesky\"][IDX][0][0])\n",
    "print(gmm_init_params[\"weights\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prec chol: 7.106993198394775. Initial mean: -0.38109374046325684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:08<03:13,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(1.9129, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:12<04:52, 12.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 23\u001b[0m\n\u001b[1;32m     16\u001b[0m torch_gmm \u001b[38;5;241m=\u001b[39m GaussianMixtureModel(\n\u001b[1;32m     17\u001b[0m     num_components\u001b[38;5;241m=\u001b[39mN_COMPONENTS,\n\u001b[1;32m     18\u001b[0m     num_features \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     19\u001b[0m     reg_covar\u001b[38;5;241m=\u001b[39mREG_COVAR,\n\u001b[1;32m     20\u001b[0m     print_idx\u001b[38;5;241m=\u001b[39mIDX\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m torch_gmm\u001b[38;5;241m.\u001b[39minitialise(gmm_init_params)\n\u001b[0;32m---> 23\u001b[0m trained_model, log_prob_epochs \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_gmm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/OpenSynth/src/opensynth/models/faraday/new_gmm/train_gmm.py:68\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, data, max_iter, convergence_tol)\u001b[0m\n\u001b[1;32m     65\u001b[0m prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[1;32m     67\u001b[0m log_prob, log_resp \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39me_step(data)\n\u001b[0;32m---> 68\u001b[0m precision_cholesky, weights, means, covariances \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_resp\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39mupdate_params(\n\u001b[1;32m     72\u001b[0m     weights\u001b[38;5;241m=\u001b[39mweights,\n\u001b[1;32m     73\u001b[0m     means\u001b[38;5;241m=\u001b[39mmeans,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     nll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlog_prob,\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Converegence\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/OpenSynth/src/opensynth/models/faraday/new_gmm/new_gmm_model.py:197\u001b[0m, in \u001b[0;36mGaussianMixtureModel.m_step\u001b[0;34m(self, X, log_reponsibilities)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mm_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor, log_reponsibilities: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    196\u001b[0m     weights_, means_, covariances_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 197\u001b[0m         \u001b[43mgmm_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_estimate_gaussian_parameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponsibilities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_reponsibilities\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreg_covar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreg_covar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     )\n\u001b[1;32m    203\u001b[0m     precision_cholesky_ \u001b[38;5;241m=\u001b[39m gmm_utils\u001b[38;5;241m.\u001b[39mtorch_compute_precision_cholesky(\n\u001b[1;32m    204\u001b[0m         covariances\u001b[38;5;241m=\u001b[39mcovariances_, reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_covar\n\u001b[1;32m    205\u001b[0m     )\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m precision_cholesky_, weights_, means_, covariances_\n",
      "File \u001b[0;32m~/Projects/OpenSynth/src/opensynth/models/faraday/new_gmm/gmm_utils.py:150\u001b[0m, in \u001b[0;36mtorch_estimate_gaussian_parameters\u001b[0;34m(X, responsibilities, reg_covar)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Compute new means usint updated responsibilities\u001b[39;00m\n\u001b[1;32m    148\u001b[0m means \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(responsibilities\u001b[38;5;241m.\u001b[39mT, X) \u001b[38;5;241m/\u001b[39m weights\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 150\u001b[0m covariances \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_compute_covariance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponsibilities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponsibilities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreg_covar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_covar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m/\u001b[39m weights\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weights, means, covariances\n",
      "File \u001b[0;32m~/Projects/OpenSynth/src/opensynth/models/faraday/new_gmm/gmm_utils.py:104\u001b[0m, in \u001b[0;36mtorch_compute_covariance\u001b[0;34m(X, means, responsibilities, weights, reg_covar)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_components):\n\u001b[1;32m    102\u001b[0m     diff \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m-\u001b[39m means_eps[k]\n\u001b[1;32m    103\u001b[0m     covariances[k] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 104\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponsibilities\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiff\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m weights[k]\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Add small regularisation\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     covariances[k] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    109\u001b[0m         covariances[k]\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(n_features, device\u001b[38;5;241m=\u001b[39mcovariances\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m reg_covar\n\u001b[1;32m    111\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from opensynth.models.faraday.new_gmm.train_gmm import initialise_gmm_params, training_loop\n",
    "from opensynth.models.faraday.new_gmm.new_gmm_model import GaussianMixtureModel\n",
    "\n",
    "\n",
    "gmm_init_params = initialise_gmm_params(\n",
    "    X=input_data,\n",
    "    n_components = N_COMPONENTS,\n",
    "    reg_covar=REG_COVAR,\n",
    ")\n",
    "\n",
    "means = gmm_init_params[\"means\"].detach().numpy()\n",
    "weights = gmm_init_params[\"weights\"].detach().numpy()\n",
    "prec_chol = gmm_init_params[\"precision_cholesky\"].detach().numpy()\n",
    "print(f\"Initial prec chol: {prec_chol[IDX][0][0]}. Initial mean: {means[IDX][0]}\")\n",
    "\n",
    "torch_gmm = GaussianMixtureModel(\n",
    "    num_components=N_COMPONENTS,\n",
    "    num_features = input_data.shape[1],\n",
    "    reg_covar=REG_COVAR,\n",
    "    print_idx=IDX\n",
    ")\n",
    "torch_gmm.initialise(gmm_init_params)\n",
    "trained_model, log_prob_epochs = training_loop(model=torch_gmm, data=input_tensor, max_iter=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK Learn GMM Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy import linalg\n",
    "\n",
    "def is_symmetric_positive_definite(covariance):\n",
    "    is_symmetric = np.all([np.allclose(covariance[i], covariance[i].T) for i in range(covariance.shape[0])])\n",
    "    is_positive_definite = np.all([np.all(np.linalg.eigvalsh(covariance[i]) > 0.0) for i in range(covariance.shape[0])])\n",
    "    return is_symmetric and is_positive_definite\n",
    "\n",
    "def _estimate_gaussian_parameters(X, resp, reg_covar=REG_COVAR):\n",
    "    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "    means = np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "    n_components, n_features = means.shape\n",
    "    covariances = np.empty((n_components, n_features, n_features))\n",
    "    for k in range(n_components):\n",
    "        diff = X - means[k]\n",
    "        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]\n",
    "        covariances[k].flat[:: n_features + 1] += reg_covar\n",
    "\n",
    "    check_covariances = is_symmetric_positive_definite(covariances)\n",
    "    if not check_covariances:\n",
    "        raise ValueError(\"Covariance matrix is not positive definite.\")\n",
    "    return nk, means, covariances\n",
    "\n",
    "def _compute_precision_cholesky(covariances):\n",
    "    estimate_precision_error_message = (\n",
    "        \"Fitting the mixture model failed because some components have \"\n",
    "        \"ill-defined empirical covariance (for instance caused by singleton \"\n",
    "        \"or collapsed samples). Try to decrease the number of components, \"\n",
    "        \"or increase reg_covar.\"\n",
    "    )\n",
    "\n",
    "    n_components, n_features, _ = covariances.shape\n",
    "    precisions_chol = np.empty((n_components, n_features, n_features))\n",
    "    for k, covariance in enumerate(covariances):\n",
    "        try:\n",
    "            cov_chol = linalg.cholesky(covariance, lower=True)\n",
    "        except linalg.LinAlgError:\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol[k] = linalg.solve_triangular(\n",
    "            cov_chol, np.eye(n_features), lower=True\n",
    "        ).T\n",
    "    return precisions_chol\n",
    "\n",
    "def _compute_log_det_cholesky(matrix_chol, n_features):\n",
    "    n_components, _, _ = matrix_chol.shape\n",
    "    log_det_chol = np.sum(\n",
    "        np.log(matrix_chol.reshape(n_components, -1)[:, :: n_features + 1]), 1\n",
    "    )\n",
    "    return log_det_chol\n",
    "\n",
    "def _estimate_log_gaussian_prob(X, means, precisions_chol):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components, _ = means.shape\n",
    "\n",
    "    log_det = _compute_log_det_cholesky(precisions_chol, n_features)\n",
    "\n",
    "    log_prob = np.empty((n_samples, n_components))\n",
    "    for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):\n",
    "        y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)\n",
    "        log_prob[:, k] = np.sum(np.square(y), axis=1)\n",
    "    return -0.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n",
    "\n",
    "def _estimate_log_weights(weights):\n",
    "        return np.log(weights)\n",
    "\n",
    "def _estimate_weighted_log_prob(X, means, precisions_chol, weights):\n",
    "        return _estimate_log_gaussian_prob(X, means, precisions_chol) + _estimate_log_weights(weights)\n",
    "\n",
    "\n",
    "def _estimate_log_prob_resp(X, means, precisions_chol, weights):\n",
    "    weighted_log_prob = _estimate_weighted_log_prob(X, means, precisions_chol, weights)\n",
    "    log_prob_norm = logsumexp(weighted_log_prob, axis=1)\n",
    "    with np.errstate(under=\"ignore\"):\n",
    "        log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]\n",
    "    return log_prob_norm, log_resp\n",
    "\n",
    "def _e_step(X,means, precisions_chol, weights):\n",
    "    log_prob_norm, log_resp = _estimate_log_prob_resp(X, means, precisions_chol, weights)\n",
    "    return np.mean(log_prob_norm), log_resp\n",
    "\n",
    "def _m_step(X, log_reponsibilities, reg_covar=REG_COVAR):\n",
    "\n",
    "    weights_, means_, covariances_ = _estimate_gaussian_parameters(X,np.exp(log_reponsibilities),reg_covar=reg_covar)\n",
    "    weights_ /= weights_.sum()\n",
    "\n",
    "    precision_cholesky_ = _compute_precision_cholesky(covariances=covariances_)\n",
    "\n",
    "    return precision_cholesky_, weights_, means_, covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prec chol: 5.304963111877441. Initial mean: -0.03158539533615112\n",
      "Old Prec Chol: 5.304963111877441. Old means: -0.03158539533615112\n",
      "New prec chol: 5.354319690242408. New means: -0.0666737731178448\n",
      "log prob: -2.034874961962169, change: inf\n",
      "Old Prec Chol: 5.354319690242408. Old means: -0.0666737731178448\n",
      "New prec chol: 5.425739612214089. New means: -0.07675008384865534\n",
      "log prob: -1.5267240808098947, change: 0.5081508811522741\n",
      "Old Prec Chol: 5.425739612214089. Old means: -0.07675008384865534\n",
      "New prec chol: 5.431166511078023. New means: -0.07761530600745582\n",
      "log prob: -1.2964411424379916, change: 0.23028293837190317\n",
      "Old Prec Chol: 5.431166511078023. Old means: -0.07761530600745582\n",
      "New prec chol: 5.447434660478251. New means: -0.0726697539764158\n",
      "log prob: -1.1458856748389143, change: 0.15055546759907723\n",
      "Old Prec Chol: 5.447434660478251. Old means: -0.0726697539764158\n",
      "New prec chol: 5.481500604338222. New means: -0.06364515597704859\n",
      "log prob: -1.0218226561414847, change: 0.12406301869742964\n",
      "Old Prec Chol: 5.481500604338222. Old means: -0.06364515597704859\n",
      "New prec chol: 5.535471568901376. New means: -0.05242268744966199\n",
      "log prob: -0.9048034885754884, change: 0.11701916756599628\n",
      "Old Prec Chol: 5.535471568901376. Old means: -0.05242268744966199\n",
      "New prec chol: 5.620235304411401. New means: -0.04071260422289683\n",
      "log prob: -0.8063830745183999, change: 0.09842041405708857\n",
      "Old Prec Chol: 5.620235304411401. Old means: -0.04071260422289683\n",
      "New prec chol: 5.734015249011019. New means: -0.0297947389607096\n",
      "log prob: -0.7343851902040726, change: 0.0719978843143273\n",
      "Old Prec Chol: 5.734015249011019. Old means: -0.0297947389607096\n",
      "New prec chol: 5.873733260572322. New means: -0.020402754928139234\n",
      "log prob: -0.682982509018508, change: 0.05140268118556457\n",
      "Old Prec Chol: 5.873733260572322. Old means: -0.020402754928139234\n",
      "New prec chol: 6.022278979611449. New means: -0.01311995879753984\n",
      "log prob: -0.6429965176586315, change: 0.03998599135987646\n",
      "Old Prec Chol: 6.022278979611449. Old means: -0.01311995879753984\n",
      "New prec chol: 6.154924968711363. New means: -0.008544173187010732\n",
      "log prob: -0.6091052511957291, change: 0.033891266462902414\n",
      "Old Prec Chol: 6.154924968711363. Old means: -0.008544173187010732\n",
      "New prec chol: 6.251948956415033. New means: -0.006184652369997113\n",
      "log prob: -0.5795171372480519, change: 0.02958811394767724\n",
      "Old Prec Chol: 6.251948956415033. Old means: -0.006184652369997113\n",
      "New prec chol: 6.313752115682206. New means: -0.004954417011561708\n",
      "log prob: -0.5538573531919824, change: 0.0256597840560695\n",
      "Old Prec Chol: 6.313752115682206. Old means: -0.004954417011561708\n",
      "New prec chol: 6.351341201591378. New means: -0.0043448711736738185\n",
      "log prob: -0.5313691088872952, change: 0.02248824430468721\n",
      "Old Prec Chol: 6.351341201591378. Old means: -0.0043448711736738185\n",
      "New prec chol: 6.374425243516772. New means: -0.004139049813677528\n",
      "log prob: -0.5127210411924125, change: 0.018648067694882697\n",
      "Old Prec Chol: 6.374425243516772. Old means: -0.004139049813677528\n",
      "New prec chol: 6.3978656444314375. New means: -0.004047949027092119\n",
      "log prob: -0.4962975209330474, change: 0.016423520259365054\n",
      "Old Prec Chol: 6.3978656444314375. Old means: -0.004047949027092119\n",
      "New prec chol: 6.430989593224513. New means: -0.003831842509813524\n",
      "log prob: -0.48204956756830986, change: 0.014247953364737553\n",
      "Old Prec Chol: 6.430989593224513. Old means: -0.003831842509813524\n",
      "New prec chol: 6.468796162028299. New means: -0.0034971642195516664\n",
      "log prob: -0.4702110778343864, change: 0.01183848973392343\n",
      "Old Prec Chol: 6.468796162028299. Old means: -0.0034971642195516664\n",
      "New prec chol: 6.509287385911478. New means: -0.0030951872792849954\n",
      "log prob: -0.45986601567970786, change: 0.010345062154678564\n",
      "Old Prec Chol: 6.509287385911478. Old means: -0.0030951872792849954\n",
      "New prec chol: 6.551072174427893. New means: -0.002572419270616087\n",
      "log prob: -0.45009070727825184, change: 0.009775308401456018\n",
      "Converged: True. Number of iterations: 19\n"
     ]
    }
   ],
   "source": [
    "means = gmm_init_params[\"means\"].detach().numpy()\n",
    "weights = gmm_init_params[\"weights\"].detach().numpy()\n",
    "prec_chol = gmm_init_params[\"precision_cholesky\"].detach().numpy()\n",
    "\n",
    "print(f\"Initial prec chol: {prec_chol[IDX][0][0]}. Initial mean: {means[IDX][0]}\")\n",
    "\n",
    "converged = False\n",
    "lower_bound = -np.inf\n",
    "np_log_prob_epochs = []\n",
    "for i in range(EPOCHS):\n",
    "    prev_lower_bound = lower_bound\n",
    "\n",
    "    print(f\"Old Prec Chol: {prec_chol[IDX][0][0]}. Old means: {means[IDX][0]}\")\n",
    "    log_prob, log_resp = _e_step(input_data, means, prec_chol, weights)\n",
    "    prec_chol, weights, means, covar = _m_step(input_data, log_resp)\n",
    "\n",
    "    print(f\"New prec chol: {prec_chol[IDX][0][0]}. New means: {means[IDX][0]}\")\n",
    "\n",
    "    # Converegence\n",
    "    lower_bound = -log_prob\n",
    "    change = abs(lower_bound - prev_lower_bound)\n",
    "    print(f\"log prob: {log_prob}, change: {change}\")\n",
    "    np_log_prob_epochs.append(lower_bound)\n",
    "    if change < CONVERGENCE_TOL:\n",
    "        converged = True\n",
    "        break\n",
    "\n",
    "print(f'Converged: {converged}. Number of iterations: {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch_log_prob_epochs = [log_prob.detach().item() for log_prob in log_prob_epochs]\n",
    "plt.plot(np.arange(0, 21), torch_log_prob_epochs, label='torch')\n",
    "# plt.plot(np.arange(0, i+1), np_log_prob_epochs, label='numpy', linestyle='--')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('log prob')\n",
    "plt.title('500 components')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK Learn GMM Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization 0\n",
      "  Iteration 1\t time lapse 7.33542s\t ll change inf\n",
      "  Iteration 2\t time lapse 3.47101s\t ll change 0.51692\n",
      "  Iteration 3\t time lapse 3.50806s\t ll change 0.23751\n",
      "  Iteration 4\t time lapse 3.10982s\t ll change 0.15756\n",
      "  Iteration 5\t time lapse 2.86738s\t ll change 0.12937\n",
      "  Iteration 6\t time lapse 3.63363s\t ll change 0.12114\n",
      "  Iteration 7\t time lapse 3.53204s\t ll change 0.10132\n",
      "  Iteration 8\t time lapse 2.65495s\t ll change 0.07272\n",
      "  Iteration 9\t time lapse 2.38858s\t ll change 0.05265\n",
      "  Iteration 10\t time lapse 2.66181s\t ll change 0.04181\n",
      "  Iteration 11\t time lapse 2.27012s\t ll change 0.03483\n",
      "  Iteration 12\t time lapse 2.43596s\t ll change 0.03022\n",
      "  Iteration 13\t time lapse 2.35215s\t ll change 0.02537\n",
      "  Iteration 14\t time lapse 2.29618s\t ll change 0.02154\n",
      "  Iteration 15\t time lapse 2.25619s\t ll change 0.01948\n",
      "  Iteration 16\t time lapse 2.35089s\t ll change 0.01818\n",
      "  Iteration 17\t time lapse 2.31402s\t ll change 0.01645\n",
      "  Iteration 18\t time lapse 2.36858s\t ll change 0.01466\n",
      "  Iteration 19\t time lapse 2.37028s\t ll change 0.01303\n",
      "  Iteration 20\t time lapse 2.19514s\t ll change 0.01193\n",
      "  Iteration 21\t time lapse 2.16888s\t ll change 0.01127\n",
      "  Iteration 22\t time lapse 2.40006s\t ll change 0.01038\n",
      "  Iteration 23\t time lapse 2.17835s\t ll change 0.01002\n",
      "  Iteration 24\t time lapse 2.24588s\t ll change 0.00932\n",
      "Initialization converged. time lapse 67.36547s\t lower bound 0.12104.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "init_weights = gmm_init_params[\"weights\"]\n",
    "init_means = gmm_init_params[\"means\"]\n",
    "\n",
    "skgmm = GaussianMixture(n_components=N_COMPONENTS, covariance_type='full', tol=CONVERGENCE_TOL, max_iter=EPOCHS, random_state=0, means_init = init_means, weights_init=init_weights, verbose=2, verbose_interval=1)\n",
    "skgmm.fit(input_data)\n",
    "skgmm_pred = skgmm.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 24)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skgmm.converged_, skgmm.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import LightningDataModule\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_tensor: torch.Tensor):\n",
    "        self.data = data_tensor\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "class CustomDataModule(LightningDataModule):\n",
    "    def __init__(self, data_tensor: torch.Tensor, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.data_tensor = data_tensor\n",
    "        self.batch_size = batch_size\n",
    "    def setup(self, stage=\"\"):\n",
    "        self.custom_ds = CustomDataset(self.data_tensor)\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.custom_ds, batch_size=self.batch_size, shuffle=False, generator=g, worker_init_fn=seed_worker)\n",
    "    \n",
    "custom_dm = CustomDataModule(data_tensor=input_tensor, batch_size=25000)\n",
    "custom_dm.setup(stage=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4973, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4973, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4973, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4973, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4973, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(5):\n",
    "    print(next(iter(custom_dm.train_dataloader()))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                      | Type                    | Params | Mode \n",
      "------------------------------------------------------------------------------\n",
      "0 | gmm_module                | GaussianMixtureModel    | 0      | train\n",
      "1 | vae_module                | FaradayVAE              | 402 K  | eval \n",
      "2 | weight_metric             | WeightsMetric           | 0      | train\n",
      "3 | mean_metric               | MeansMetric             | 0      | train\n",
      "4 | precision_cholesky_metric | PrecisionCholeskyMetric | 0      | train\n",
      "5 | covariance_metric         | CovarianceMetric        | 0      | train\n",
      "6 | nll                       | NLLMetric               | 0      | train\n",
      "------------------------------------------------------------------------------\n",
      "402 K     Trainable params\n",
      "0         Non-trainable params\n",
      "402 K     Total params\n",
      "1.609     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "32        Modules in eval mode\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prec chol: 8.24792194366455. Initial mean: -0.4213249683380127\n",
      "Epoch 0: 100%|██████████| 4/4 [00:06<00:00,  0.58it/s, v_num=88]Local weights at rank: 0 - means: 0.0056, -0.4014\n",
      "Reduced weights, means, covar: 0.0056, -0.4014, 0.0061\n",
      "log prob:  tensor(3.0380)\n",
      "Epoch 1: 100%|██████████| 4/4 [00:06<00:00,  0.62it/s, v_num=88]Local weights at rank: 0 - means: 0.0045, -0.3822\n",
      "Reduced weights, means, covar: 0.0045, -0.3822, 0.0047\n",
      "log prob:  tensor(2.4274)\n",
      "Epoch 2: 100%|██████████| 4/4 [00:05<00:00,  0.67it/s, v_num=88]Local weights at rank: 0 - means: 0.0060, -0.3719\n",
      "Reduced weights, means, covar: 0.0060, -0.3719, 0.0035\n",
      "log prob:  tensor(2.2690)\n",
      "Epoch 3: 100%|██████████| 4/4 [00:05<00:00,  0.71it/s, v_num=88]Local weights at rank: 0 - means: 0.0066, -0.3770\n",
      "Reduced weights, means, covar: 0.0066, -0.3770, 0.0033\n",
      "log prob:  tensor(2.2098)\n",
      "Epoch 4: 100%|██████████| 4/4 [00:05<00:00,  0.74it/s, v_num=88]Local weights at rank: 0 - means: 0.0074, -0.3834\n",
      "Reduced weights, means, covar: 0.0074, -0.3834, 0.0034\n",
      "log prob:  tensor(2.1310)\n",
      "Epoch 5: 100%|██████████| 4/4 [00:05<00:00,  0.73it/s, v_num=88]Local weights at rank: 0 - means: 0.0073, -0.3905\n",
      "Reduced weights, means, covar: 0.0073, -0.3905, 0.0035\n",
      "log prob:  tensor(2.0579)\n",
      "Epoch 6: 100%|██████████| 4/4 [00:05<00:00,  0.71it/s, v_num=88]Local weights at rank: 0 - means: 0.0071, -0.3960\n",
      "Reduced weights, means, covar: 0.0071, -0.3960, 0.0036\n",
      "log prob:  tensor(1.9926)\n",
      "Epoch 7: 100%|██████████| 4/4 [00:06<00:00,  0.59it/s, v_num=88]Local weights at rank: 0 - means: 0.0069, -0.3979\n",
      "Reduced weights, means, covar: 0.0069, -0.3979, 0.0036\n",
      "log prob:  tensor(1.9472)\n",
      "Epoch 8: 100%|██████████| 4/4 [00:06<00:00,  0.65it/s, v_num=88]Local weights at rank: 0 - means: 0.0070, -0.3987\n",
      "Reduced weights, means, covar: 0.0070, -0.3987, 0.0037\n",
      "log prob:  tensor(1.9162)\n",
      "Epoch 9: 100%|██████████| 4/4 [00:05<00:00,  0.68it/s, v_num=88]Local weights at rank: 0 - means: 0.0073, -0.4000\n",
      "Reduced weights, means, covar: 0.0073, -0.4000, 0.0036\n",
      "log prob:  tensor(1.9078)\n",
      "Epoch 9: 100%|██████████| 4/4 [00:05<00:00,  0.68it/s, v_num=88]\n"
     ]
    }
   ],
   "source": [
    "from opensynth.models.faraday.new_gmm.new_gmm_model import GaussianMixtureLightningModule, GaussianMixtureModel\n",
    "gmm_module = GaussianMixtureModel(\n",
    "    num_components=N_COMPONENTS,\n",
    "    num_features = input_data.shape[1],\n",
    "    reg_covar=REG_COVAR,\n",
    "    print_idx=IDX\n",
    ")\n",
    "gmm_module.initialise(gmm_init_params)\n",
    "print(f\"Initial prec chol: {gmm_module.precision_cholesky[IDX][0][0]}. Initial mean: {gmm_module.means[IDX][0]}\")\n",
    "\n",
    "gmm_lightning_module = GaussianMixtureLightningModule(\n",
    "    gmm_module = gmm_module,\n",
    "    vae_module = vae_model,\n",
    "    num_components = gmm_module.num_components,\n",
    "    num_features = gmm_module.num_features,\n",
    "    reg_covar = gmm_module.reg_covar,\n",
    "    convergence_tolerance = CONVERGENCE_TOL,\n",
    "    compute_on_batch=False\n",
    ")\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, accelerator=\"cpu\", deterministic=True )\n",
    "trainer.fit(gmm_lightning_module, custom_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sk-learn GMM batch learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number:  0\n",
      "means:  [[-0.29049615 -1.8366832   0.57180723 ... -1.77699099  8.23024724\n",
      "   3.71545809]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.6029433  -1.25886529  0.50056563 ... -0.92390276  4.5986303\n",
      "   3.4323372 ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "Batch number:  1\n",
      "means:  [[-0.28136729 -1.87688303  0.55990058 ... -1.72341048  7.01239623\n",
      "   3.07135017]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.6195133  -1.07679951  0.46372143 ... -0.87361805  6.48789928\n",
      "   2.87193667]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "Batch number:  2\n",
      "means:  [[-0.27579347 -1.8148005   0.52173095 ... -1.63402786  7.14378599\n",
      "   3.31941643]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.64880977 -1.16273321  0.57315852 ... -0.9128379   6.37736483\n",
      "   2.75477067]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "Batch number:  3\n",
      "means:  [[-0.28269624 -1.77667548  0.49735797 ... -1.61740005  7.39031996\n",
      "   3.07082778]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.5851443  -1.13857955  0.55146234 ... -0.98918665  6.43659724\n",
      "   2.62936034]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "init_weights = gmm_init_params[\"weights\"]\n",
    "init_means = gmm_init_params[\"means\"]\n",
    "\n",
    "skgmm = GaussianMixture(n_components=N_COMPONENTS, covariance_type='full', tol=CONVERGENCE_TOL, max_iter=EPOCHS, random_state=0, means_init = init_means, weights_init=init_weights, warm_start=True)\n",
    "\n",
    "dl = custom_dm.train_dataloader()\n",
    "next_batch = next(iter(dl))\n",
    "for batch_num, batch_data in enumerate(dl):\n",
    "    print(\"Batch number: \", batch_num)\n",
    "    input_data = batch_data.detach().numpy()\n",
    "    n_samples = len(input_tensor)\n",
    "    skgmm.fit(input_data)\n",
    "    print(\"means: \", skgmm.means_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4000, -2.1992,  0.9284,  ..., -2.3797,  6.8579,  4.3441],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.4042, -2.2261,  0.5635,  ..., -1.8238,  2.7293,  3.6901],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_lightning_module.gmm_module.means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28269624, -1.77667548,  0.49735797, ..., -1.61740005,\n",
       "         7.39031996,  3.07082778],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.5851443 , -1.13857955,  0.55146234, ..., -0.98918665,\n",
       "         6.43659724,  2.62936034],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skgmm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(613, 557)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_sum_components = skgmm.means_.sum(axis=1)\n",
    "ligthning_sum_components = gmm_lightning_module.gmm_module.means.sum(axis=1)\n",
    "len(sklearn_sum_components[sklearn_sum_components==0]), len(ligthning_sum_components[ligthning_sum_components==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0303), tensor(-0.0037))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_lightning_module.gmm_module.weights[0], gmm_lightning_module.gmm_module.means[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0303), tensor(-0.0037))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_lightning_module.gmm_module.weights[0], gmm_lightning_module.gmm_module.means[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0303), tensor(-0.0037))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_lightning_module.weight_metric.compute()[0], gmm_lightning_module.mean_metric.compute()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skgmm</th>\n",
       "      <th>numpy</th>\n",
       "      <th>torch</th>\n",
       "      <th>lightning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.049669</td>\n",
       "      <td>-0.002572</td>\n",
       "      <td>-0.003688</td>\n",
       "      <td>-0.003688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.707486</td>\n",
       "      <td>-2.928096</td>\n",
       "      <td>-2.921445</td>\n",
       "      <td>-2.921445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.303743</td>\n",
       "      <td>0.182399</td>\n",
       "      <td>0.184473</td>\n",
       "      <td>0.184473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.109831</td>\n",
       "      <td>0.073859</td>\n",
       "      <td>0.069771</td>\n",
       "      <td>0.069771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012985</td>\n",
       "      <td>0.100071</td>\n",
       "      <td>0.096574</td>\n",
       "      <td>0.096574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.090285</td>\n",
       "      <td>0.095880</td>\n",
       "      <td>0.095612</td>\n",
       "      <td>0.095612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.532626</td>\n",
       "      <td>0.414696</td>\n",
       "      <td>0.416915</td>\n",
       "      <td>0.416915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.637247</td>\n",
       "      <td>1.820193</td>\n",
       "      <td>1.817437</td>\n",
       "      <td>1.817437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.010636</td>\n",
       "      <td>-0.035010</td>\n",
       "      <td>-0.034580</td>\n",
       "      <td>-0.034580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.646689</td>\n",
       "      <td>-1.858131</td>\n",
       "      <td>-1.849922</td>\n",
       "      <td>-1.849922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.697293</td>\n",
       "      <td>0.627190</td>\n",
       "      <td>0.628446</td>\n",
       "      <td>0.628446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.083588</td>\n",
       "      <td>-0.082038</td>\n",
       "      <td>-0.079670</td>\n",
       "      <td>-0.079670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.325839</td>\n",
       "      <td>0.272189</td>\n",
       "      <td>0.270991</td>\n",
       "      <td>0.270991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.726988</td>\n",
       "      <td>0.787408</td>\n",
       "      <td>0.787077</td>\n",
       "      <td>0.787077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.233333</td>\n",
       "      <td>1.325930</td>\n",
       "      <td>1.321312</td>\n",
       "      <td>1.321312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-2.198755</td>\n",
       "      <td>-2.365580</td>\n",
       "      <td>-2.359361</td>\n",
       "      <td>-2.359361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.938204</td>\n",
       "      <td>6.913948</td>\n",
       "      <td>6.912699</td>\n",
       "      <td>6.912699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.810847</td>\n",
       "      <td>2.738045</td>\n",
       "      <td>2.739079</td>\n",
       "      <td>2.739079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       skgmm     numpy     torch  lightning\n",
       "0  -0.049669 -0.002572 -0.003688  -0.003688\n",
       "1  -2.707486 -2.928096 -2.921445  -2.921445\n",
       "2   0.303743  0.182399  0.184473   0.184473\n",
       "3  -0.109831  0.073859  0.069771   0.069771\n",
       "4   0.012985  0.100071  0.096574   0.096574\n",
       "5   0.090285  0.095880  0.095612   0.095612\n",
       "6   0.532626  0.414696  0.416915   0.416915\n",
       "7   1.637247  1.820193  1.817437   1.817437\n",
       "8  -0.010636 -0.035010 -0.034580  -0.034580\n",
       "9  -1.646689 -1.858131 -1.849922  -1.849922\n",
       "10  0.697293  0.627190  0.628446   0.628446\n",
       "11  0.083588 -0.082038 -0.079670  -0.079670\n",
       "12  0.325839  0.272189  0.270991   0.270991\n",
       "13  0.726988  0.787408  0.787077   0.787077\n",
       "14  1.233333  1.325930  1.321312   1.321312\n",
       "15 -2.198755 -2.365580 -2.359361  -2.359361\n",
       "16  6.938204  6.913948  6.912699   6.912699\n",
       "17  2.810847  2.738045  2.739079   2.739079"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare_means = pd.DataFrame()\n",
    "df_compare_means[\"skgmm\"] = skgmm.means_[IDX]\n",
    "df_compare_means[\"numpy\"] = means[IDX]\n",
    "df_compare_means[\"torch\"] = trained_model.means[IDX]\n",
    "df_compare_means[\"lightning\"] = gmm_lightning_module.gmm_module.means[IDX]\n",
    "df_compare_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0316, -2.8152,  0.0335,  0.3134,  0.1846,  0.0512,  0.2501,  2.1094,\n",
       "        -0.0596, -2.1234,  0.4170, -0.0876, -0.0460,  0.7939,  1.4049, -2.3318,\n",
       "         4.5358,  1.2399])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_init_params[\"means\"][IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skgmm</th>\n",
       "      <th>numpy</th>\n",
       "      <th>torch</th>\n",
       "      <th>lightning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017656</td>\n",
       "      <td>0.023301</td>\n",
       "      <td>0.023192</td>\n",
       "      <td>0.023192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.066879</td>\n",
       "      <td>-0.087802</td>\n",
       "      <td>-0.087399</td>\n",
       "      <td>-0.087399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.020881</td>\n",
       "      <td>-0.023410</td>\n",
       "      <td>-0.023240</td>\n",
       "      <td>-0.023240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029825</td>\n",
       "      <td>0.038741</td>\n",
       "      <td>0.038555</td>\n",
       "      <td>0.038555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021761</td>\n",
       "      <td>0.038529</td>\n",
       "      <td>0.038498</td>\n",
       "      <td>0.038498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.007635</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>0.011275</td>\n",
       "      <td>0.011275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.016465</td>\n",
       "      <td>-0.015620</td>\n",
       "      <td>-0.015511</td>\n",
       "      <td>-0.015511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.016911</td>\n",
       "      <td>0.018662</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.018408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.003483</td>\n",
       "      <td>-0.003759</td>\n",
       "      <td>-0.003729</td>\n",
       "      <td>-0.003729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.036252</td>\n",
       "      <td>-0.053979</td>\n",
       "      <td>-0.053962</td>\n",
       "      <td>-0.053962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.002098</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.001633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.048491</td>\n",
       "      <td>-0.062710</td>\n",
       "      <td>-0.062302</td>\n",
       "      <td>-0.062302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.015314</td>\n",
       "      <td>0.017880</td>\n",
       "      <td>0.017944</td>\n",
       "      <td>0.017944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.014510</td>\n",
       "      <td>0.019040</td>\n",
       "      <td>0.018825</td>\n",
       "      <td>0.018825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.038426</td>\n",
       "      <td>0.038441</td>\n",
       "      <td>0.038441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.047407</td>\n",
       "      <td>-0.065697</td>\n",
       "      <td>-0.065515</td>\n",
       "      <td>-0.065515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.040891</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.034912</td>\n",
       "      <td>0.034912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.018379</td>\n",
       "      <td>0.038758</td>\n",
       "      <td>0.038364</td>\n",
       "      <td>0.038364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       skgmm     numpy     torch  lightning\n",
       "0   0.017656  0.023301  0.023192   0.023192\n",
       "1  -0.066879 -0.087802 -0.087399  -0.087399\n",
       "2  -0.020881 -0.023410 -0.023240  -0.023240\n",
       "3   0.029825  0.038741  0.038555   0.038555\n",
       "4   0.021761  0.038529  0.038498   0.038498\n",
       "5   0.007635  0.011312  0.011275   0.011275\n",
       "6  -0.016465 -0.015620 -0.015511  -0.015511\n",
       "7   0.016911  0.018662  0.018408   0.018408\n",
       "8  -0.003483 -0.003759 -0.003729  -0.003729\n",
       "9  -0.036252 -0.053979 -0.053962  -0.053962\n",
       "10 -0.002098  0.001644  0.001633   0.001633\n",
       "11 -0.048491 -0.062710 -0.062302  -0.062302\n",
       "12  0.015314  0.017880  0.017944   0.017944\n",
       "13  0.014510  0.019040  0.018825   0.018825\n",
       "14  0.020142  0.038426  0.038441   0.038441\n",
       "15 -0.047407 -0.065697 -0.065515  -0.065515\n",
       "16  0.040891  0.033842  0.034912   0.034912\n",
       "17  0.018379  0.038758  0.038364   0.038364"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare_covar = pd.DataFrame()\n",
    "df_compare_covar[\"skgmm\"] = skgmm.covariances_[IDX][0]\n",
    "df_compare_covar[\"numpy\"] = covar[IDX][0]\n",
    "df_compare_covar[\"torch\"] = trained_model.covariances.detach().numpy()[IDX][0]\n",
    "df_compare_covar[\"lightning\"] = gmm_lightning_module.gmm_module.covariances.detach().numpy()[IDX][0]\n",
    "df_compare_covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skgmm</th>\n",
       "      <th>numpy</th>\n",
       "      <th>torch</th>\n",
       "      <th>lightning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.525883</td>\n",
       "      <td>6.551072</td>\n",
       "      <td>6.566400</td>\n",
       "      <td>6.566400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.392054</td>\n",
       "      <td>5.355093</td>\n",
       "      <td>5.367649</td>\n",
       "      <td>5.367649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.217989</td>\n",
       "      <td>6.399017</td>\n",
       "      <td>6.407043</td>\n",
       "      <td>6.407043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.754001</td>\n",
       "      <td>-3.943360</td>\n",
       "      <td>-3.980647</td>\n",
       "      <td>-3.980647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-7.994878</td>\n",
       "      <td>-6.608500</td>\n",
       "      <td>-6.620760</td>\n",
       "      <td>-6.620760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-11.216321</td>\n",
       "      <td>-8.930598</td>\n",
       "      <td>-8.937640</td>\n",
       "      <td>-8.937640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.903968</td>\n",
       "      <td>5.665672</td>\n",
       "      <td>5.663373</td>\n",
       "      <td>5.663373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23.009691</td>\n",
       "      <td>13.939803</td>\n",
       "      <td>13.997698</td>\n",
       "      <td>13.997698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.872313</td>\n",
       "      <td>-1.436411</td>\n",
       "      <td>-1.420174</td>\n",
       "      <td>-1.420174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-17.834390</td>\n",
       "      <td>-14.562229</td>\n",
       "      <td>-14.590742</td>\n",
       "      <td>-14.590742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.223420</td>\n",
       "      <td>4.728659</td>\n",
       "      <td>4.809395</td>\n",
       "      <td>4.809395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.581919</td>\n",
       "      <td>10.362807</td>\n",
       "      <td>10.370152</td>\n",
       "      <td>10.370152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-3.528971</td>\n",
       "      <td>-0.999375</td>\n",
       "      <td>-0.998038</td>\n",
       "      <td>-0.998038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-9.912942</td>\n",
       "      <td>-4.954043</td>\n",
       "      <td>-5.014349</td>\n",
       "      <td>-5.014349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.209757</td>\n",
       "      <td>1.505791</td>\n",
       "      <td>1.501181</td>\n",
       "      <td>1.501181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-2.718472</td>\n",
       "      <td>-1.603686</td>\n",
       "      <td>-1.615772</td>\n",
       "      <td>-1.615772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-3.557216</td>\n",
       "      <td>-1.072163</td>\n",
       "      <td>-1.044492</td>\n",
       "      <td>-1.044492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.656508</td>\n",
       "      <td>-0.681579</td>\n",
       "      <td>-0.686414</td>\n",
       "      <td>-0.686414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        skgmm      numpy      torch  lightning\n",
       "0    7.525883   6.551072   6.566400   6.566400\n",
       "1    6.392054   5.355093   5.367649   5.367649\n",
       "2    8.217989   6.399017   6.407043   6.407043\n",
       "3   -4.754001  -3.943360  -3.980647  -3.980647\n",
       "4   -7.994878  -6.608500  -6.620760  -6.620760\n",
       "5  -11.216321  -8.930598  -8.937640  -8.937640\n",
       "6    4.903968   5.665672   5.663373   5.663373\n",
       "7   23.009691  13.939803  13.997698  13.997698\n",
       "8   -1.872313  -1.436411  -1.420174  -1.420174\n",
       "9  -17.834390 -14.562229 -14.590742 -14.590742\n",
       "10   8.223420   4.728659   4.809395   4.809395\n",
       "11  11.581919  10.362807  10.370152  10.370152\n",
       "12  -3.528971  -0.999375  -0.998038  -0.998038\n",
       "13  -9.912942  -4.954043  -5.014349  -5.014349\n",
       "14   3.209757   1.505791   1.501181   1.501181\n",
       "15  -2.718472  -1.603686  -1.615772  -1.615772\n",
       "16  -3.557216  -1.072163  -1.044492  -1.044492\n",
       "17  -1.656508  -0.681579  -0.686414  -0.686414"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare_pre_chol = pd.DataFrame()\n",
    "df_compare_pre_chol[\"skgmm\"] = skgmm.precisions_cholesky_[IDX][0]\n",
    "df_compare_pre_chol[\"numpy\"] = prec_chol[IDX][0]\n",
    "df_compare_pre_chol[\"torch\"] = trained_model.precision_cholesky.detach().numpy()[IDX][0]\n",
    "df_compare_pre_chol[\"lightning\"] = gmm_lightning_module.gmm_module.precision_cholesky.detach().numpy()[IDX][0]\n",
    "df_compare_pre_chol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skgmm</th>\n",
       "      <th>numpy</th>\n",
       "      <th>torch</th>\n",
       "      <th>lightning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028506</td>\n",
       "      <td>0.030255</td>\n",
       "      <td>0.030305</td>\n",
       "      <td>0.030305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.004476</td>\n",
       "      <td>0.004478</td>\n",
       "      <td>0.004478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003403</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.003295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.013637</td>\n",
       "      <td>0.011810</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.011803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.002256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.002665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      skgmm     numpy     torch  lightning\n",
       "0  0.028506  0.030255  0.030305   0.030305\n",
       "1  0.000200  0.000200  0.000200   0.000200\n",
       "2  0.004717  0.004476  0.004478   0.004478\n",
       "3  0.003403  0.003295  0.003295   0.003295\n",
       "4  0.000600  0.000600  0.000600   0.000600\n",
       "5  0.013637  0.011810  0.011803   0.011803\n",
       "6  0.000320  0.000320  0.000320   0.000320\n",
       "7  0.002364  0.002256  0.002256   0.002256\n",
       "8  0.002725  0.002665  0.002665   0.002665\n",
       "9  0.000240  0.000240  0.000240   0.000240"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare_weights = pd.DataFrame()\n",
    "df_compare_weights[\"skgmm\"] = skgmm.weights_[:10]\n",
    "df_compare_weights[\"numpy\"] = weights[:10]\n",
    "df_compare_weights[\"torch\"] = trained_model.weights[:10]\n",
    "df_compare_weights[\"lightning\"] = gmm_lightning_module.gmm_module.weights.detach().numpy()[:10]\n",
    "df_compare_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(means_, covariances_, weights_, n_samples):\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    n_samples_comp = rng.multinomial(n_samples, weights_)\n",
    "    \n",
    "    X = np.vstack(\n",
    "            [\n",
    "                rng.multivariate_normal(mean, covariance, int(sample))\n",
    "                for (mean, covariance, sample) in zip(\n",
    "                    means_, covariances_, n_samples_comp\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    y = np.concatenate(\n",
    "        [np.full(sample, j, dtype=int) for j, sample in enumerate(n_samples_comp)]\n",
    "    )\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_sample(means_, covariances_, weights_, n_samples):\n",
    "    # Set up the random generator with a specified seed\n",
    "    generator = torch.Generator().manual_seed(RANDOM_STATE)\n",
    "    \n",
    "    # Sample component counts from the multinomial distribution\n",
    "    n_samples_comp = torch.multinomial(weights_, n_samples, replacement=True, generator=generator).bincount(minlength=len(weights_))\n",
    "    \n",
    "    # Initialize lists to collect samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Sample from each component based on the number of samples\n",
    "    for j, (mean, covariance, sample_count) in enumerate(zip(means_, covariances_, n_samples_comp)):\n",
    "        if sample_count > 0:  # Only sample if we need samples from this component\n",
    "            dist = torch.distributions.MultivariateNormal(\n",
    "                mean, covariance\n",
    "            )\n",
    "            samples = dist.sample((sample_count,))\n",
    "            X.append(samples)\n",
    "            y.append(torch.full((sample_count,), j, dtype=torch.int64))\n",
    "    \n",
    "    # Concatenate all samples and labels into single tensors\n",
    "    X = torch.vstack(X)\n",
    "    y = torch.cat(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.03887747, -2.9453815 ,  0.43721818, -0.20162165,  0.28700438,\n",
       "         0.1723324 ,  0.81485445,  1.7054237 ,  0.08203473, -1.72305909,\n",
       "         1.00713371,  0.04058859,  0.19500022,  1.00962552,  1.61513841,\n",
       "        -2.42326199, 13.6384044 ,  3.04574583]),\n",
       " 0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skgmm_samples = sample(skgmm.means_, skgmm.covariances_, skgmm.weights_, n_samples = N_SAMPLES)\n",
    "\n",
    "skgmm_X, skgmm_y = skgmm_samples\n",
    "skgmm_X[IDX], skgmm_y[IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.32496524e-02, -2.98268887e+00,  4.24690527e-01, -1.67278872e-01,\n",
       "         2.24809896e-01,  1.06553346e-01,  6.26317064e-01,  1.26015957e+00,\n",
       "         2.01741825e-03, -2.07779531e+00,  7.85901517e-01,  2.26804101e-01,\n",
       "         8.27804447e-01,  5.43464053e-01,  1.54425686e+00, -2.70189306e+00,\n",
       "         1.33966804e+01,  3.17510242e+00]),\n",
       " 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = sample(means, covar, weights, n_samples = N_SAMPLES)\n",
    "\n",
    "X, y = samples\n",
    "X[IDX], y[IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0756, -3.2072,  0.2560,  0.1855,  1.0395,  0.4390,  0.9301,  2.0937,\n",
       "         -0.0612, -2.1749,  1.0848, -0.6334, -0.5247,  1.3095,  2.3578, -2.5904,\n",
       "          5.2245,  2.1991]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_samples = torch_sample(trained_model.means, trained_model.covariances, trained_model.weights, n_samples = N_SAMPLES)\n",
    "train_model_X, train_model_y = train_model_samples\n",
    "train_model_X[IDX], train_model_y[IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0756, -3.2072,  0.2560,  0.1855,  1.0395,  0.4390,  0.9301,  2.0937,\n",
       "         -0.0612, -2.1749,  1.0848, -0.6334, -0.5247,  1.3095,  2.3578, -2.5904,\n",
       "          5.2245,  2.1991]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_lightning_samples = torch_sample(gmm_lightning_module.gmm_module.means, gmm_lightning_module.gmm_module.covariances, gmm_lightning_module.gmm_module.weights, n_samples = N_SAMPLES)\n",
    "gmm_lightning_X, gmm_lightning_y = train_model_samples\n",
    "gmm_lightning_X[IDX], gmm_lightning_y[IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenSynth-BNsxhSIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
