{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "full_data = pd.read_csv(\"../../data/processed/historical/train/lcl_data.csv\")\n",
    "df_100K = full_data.sample(100000, random_state=0)\n",
    "df_100K.to_csv(\"../../data/processed/historical/train/lcl_data_100K.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "RANDOM_STATE = 0\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(RANDOM_STATE)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from opensynth.data_modules.lcl_data_module import LCLDataModule\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = Path(\"../../data/processed/historical/train/lcl_data_100K.csv\")\n",
    "stats_path = Path(\"../../data/processed/historical/train/mean_std.csv\")\n",
    "outlier_path = Path(\"../../data/processed/historical/train/outliers.csv\")\n",
    "\n",
    "dm = LCLDataModule(data_path=data_path, stats_path=stats_path, batch_size=25000, n_samples=100000)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FaradayVAE(\n",
       "  (encoder): Encoder(\n",
       "    (encoder_layers): Sequential(\n",
       "      (0): Linear(in_features=50, out_features=512, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "      (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (7): GELU(approximate='none')\n",
       "      (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (9): GELU(approximate='none')\n",
       "      (10): Linear(in_features=32, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (latent): Linear(in_features=18, out_features=16, bias=True)\n",
       "    (latent_activations): GELU(approximate='none')\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "      (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (7): GELU(approximate='none')\n",
       "      (8): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (9): GELU(approximate='none')\n",
       "      (10): Linear(in_features=512, out_features=48, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (reparametriser): ReparametrisationModule(\n",
       "    (mean): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (logvar): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from opensynth.models.faraday import FaradayVAE\n",
    "vae_model = torch.load(\"vae_model.pt\")\n",
    "vae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensynth.models.faraday.gaussian_mixture.prepare_gmm_input import encode_data_for_gmm\n",
    "\n",
    "next_batch = next(iter(dm.train_dataloader()))\n",
    "input_tensor = encode_data_for_gmm(data=next_batch, vae_module=vae_model)\n",
    "input_data = input_tensor.detach().numpy()\n",
    "n_samples = len(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COMPONENTS = 700\n",
    "REG_COVAR = 1e-4\n",
    "EPOCHS = 25\n",
    "IDX = 0\n",
    "CONVERGENCE_TOL = 1e-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25000, 18]), tensor(0.4973, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, input_tensor[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "from opensynth.models.faraday.new_gmm import gmm_utils\n",
    "\n",
    "labels_, means_, responsibilities_ = gmm_utils.initialise_centroids(\n",
    "        X=input_data, n_components=N_COMPONENTS\n",
    "    )\n",
    "print(labels_.dtype, responsibilities_.dtype, means_.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6420)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "from opensynth.models.faraday.new_gmm.train_gmm import initialise_gmm_params\n",
    "\n",
    "gmm_init_params = initialise_gmm_params(\n",
    "    X=input_data,\n",
    "    n_components = N_COMPONENTS,\n",
    "    reg_covar=REG_COVAR,\n",
    ")\n",
    "print(gmm_init_params[\"precision_cholesky\"][IDX][0][0])\n",
    "print(gmm_init_params[\"weights\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prec chol: 6.642029285430908. Initial mean: -0.11744648963212967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:02<01:06,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-1.7353, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:05<01:05,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.1089, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [00:08<00:57,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.3179, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [00:10<00:52,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.4595, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:12<00:50,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.5652, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:15<00:49,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.6564, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [00:18<00:48,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.7336, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [00:21<00:46,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.7934, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [00:24<00:45,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.8381, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [00:27<00:45,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.8720, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [00:31<00:44,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.8965, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [00:34<00:41,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.9157, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [00:37<00:37,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.9323, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [00:41<00:37,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.9483, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [00:44<00:32,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.9611, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [00:47<00:27,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.9725, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [00:49<00:23,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.9835, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [00:52<00:24,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob:  tensor(-2.9934, grad_fn=<NegBackward0>)\n",
      "Converged: True. Number of iterations: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from opensynth.models.faraday.new_gmm.train_gmm import initialise_gmm_params, training_loop\n",
    "from opensynth.models.faraday.new_gmm.new_gmm_model import GaussianMixtureModel\n",
    "\n",
    "\n",
    "gmm_init_params = initialise_gmm_params(\n",
    "    X=input_data,\n",
    "    n_components = N_COMPONENTS,\n",
    "    reg_covar=REG_COVAR,\n",
    ")\n",
    "\n",
    "means = gmm_init_params[\"means\"].detach().numpy()\n",
    "weights = gmm_init_params[\"weights\"].detach().numpy()\n",
    "prec_chol = gmm_init_params[\"precision_cholesky\"].detach().numpy()\n",
    "print(f\"Initial prec chol: {prec_chol[IDX][0][0]}. Initial mean: {means[IDX][0]}\")\n",
    "\n",
    "torch_gmm = GaussianMixtureModel(\n",
    "    num_components=N_COMPONENTS,\n",
    "    num_features = input_data.shape[1],\n",
    "    reg_covar=REG_COVAR,\n",
    "    print_idx=IDX\n",
    ")\n",
    "torch_gmm.initialise(gmm_init_params)\n",
    "trained_model, log_prob_epochs = training_loop(model=torch_gmm, data=input_tensor, max_iter=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK Learn GMM Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy import linalg\n",
    "\n",
    "def is_symmetric_positive_definite(covariance):\n",
    "    is_symmetric = np.all([np.allclose(covariance[i], covariance[i].T) for i in range(covariance.shape[0])])\n",
    "    is_positive_definite = np.all([np.all(np.linalg.eigvalsh(covariance[i]) > 0.0) for i in range(covariance.shape[0])])\n",
    "    return is_symmetric and is_positive_definite\n",
    "\n",
    "def _estimate_gaussian_parameters(X, resp, reg_covar=REG_COVAR):\n",
    "    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "    means = np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "    n_components, n_features = means.shape\n",
    "    covariances = np.empty((n_components, n_features, n_features))\n",
    "    for k in range(n_components):\n",
    "        diff = X - means[k]\n",
    "        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]\n",
    "        covariances[k].flat[:: n_features + 1] += reg_covar\n",
    "\n",
    "    check_covariances = is_symmetric_positive_definite(covariances)\n",
    "    if not check_covariances:\n",
    "        raise ValueError(\"Covariance matrix is not positive definite.\")\n",
    "    return nk, means, covariances\n",
    "\n",
    "def _compute_precision_cholesky(covariances):\n",
    "    estimate_precision_error_message = (\n",
    "        \"Fitting the mixture model failed because some components have \"\n",
    "        \"ill-defined empirical covariance (for instance caused by singleton \"\n",
    "        \"or collapsed samples). Try to decrease the number of components, \"\n",
    "        \"or increase reg_covar.\"\n",
    "    )\n",
    "\n",
    "    n_components, n_features, _ = covariances.shape\n",
    "    precisions_chol = np.empty((n_components, n_features, n_features))\n",
    "    for k, covariance in enumerate(covariances):\n",
    "        try:\n",
    "            cov_chol = linalg.cholesky(covariance, lower=True)\n",
    "        except linalg.LinAlgError:\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol[k] = linalg.solve_triangular(\n",
    "            cov_chol, np.eye(n_features), lower=True\n",
    "        ).T\n",
    "    return precisions_chol\n",
    "\n",
    "def _compute_log_det_cholesky(matrix_chol, n_features):\n",
    "    n_components, _, _ = matrix_chol.shape\n",
    "    log_det_chol = np.sum(\n",
    "        np.log(matrix_chol.reshape(n_components, -1)[:, :: n_features + 1]), 1\n",
    "    )\n",
    "    return log_det_chol\n",
    "\n",
    "def _estimate_log_gaussian_prob(X, means, precisions_chol):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components, _ = means.shape\n",
    "\n",
    "    log_det = _compute_log_det_cholesky(precisions_chol, n_features)\n",
    "\n",
    "    log_prob = np.empty((n_samples, n_components))\n",
    "    for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):\n",
    "        y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)\n",
    "        log_prob[:, k] = np.sum(np.square(y), axis=1)\n",
    "    return -0.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n",
    "\n",
    "def _estimate_log_weights(weights):\n",
    "        return np.log(weights)\n",
    "\n",
    "def _estimate_weighted_log_prob(X, means, precisions_chol, weights):\n",
    "        return _estimate_log_gaussian_prob(X, means, precisions_chol) + _estimate_log_weights(weights)\n",
    "\n",
    "\n",
    "def _estimate_log_prob_resp(X, means, precisions_chol, weights):\n",
    "    weighted_log_prob = _estimate_weighted_log_prob(X, means, precisions_chol, weights)\n",
    "    log_prob_norm = logsumexp(weighted_log_prob, axis=1)\n",
    "    with np.errstate(under=\"ignore\"):\n",
    "        log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]\n",
    "    return log_prob_norm, log_resp\n",
    "\n",
    "def _e_step(X,means, precisions_chol, weights):\n",
    "    log_prob_norm, log_resp = _estimate_log_prob_resp(X, means, precisions_chol, weights)\n",
    "    return np.mean(log_prob_norm), log_resp\n",
    "\n",
    "def _m_step(X, log_reponsibilities, reg_covar=REG_COVAR):\n",
    "\n",
    "    weights_, means_, covariances_ = _estimate_gaussian_parameters(X,np.exp(log_reponsibilities),reg_covar=reg_covar)\n",
    "    weights_ /= weights_.sum()\n",
    "\n",
    "    precision_cholesky_ = _compute_precision_cholesky(covariances=covariances_)\n",
    "\n",
    "    return precision_cholesky_, weights_, means_, covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prec chol: 6.642029285430908. Initial mean: -0.11744648963212967\n",
      "Old Prec Chol: 6.642029285430908. Old means: -0.11744648963212967\n",
      "New prec chol: 6.432963578683903. New means: -0.13688528732254718\n",
      "log prob: 1.735338299854014, change: inf\n",
      "Old Prec Chol: 6.432963578683903. Old means: -0.13688528732254718\n",
      "New prec chol: 6.423680953707583. New means: -0.15136358003929168\n",
      "log prob: 2.108812957430841, change: 0.3734746575768271\n",
      "Old Prec Chol: 6.423680953707583. Old means: -0.15136358003929168\n",
      "New prec chol: 6.537481563374545. New means: -0.161987004486275\n",
      "log prob: 2.3177299763142356, change: 0.20891701888339442\n",
      "Old Prec Chol: 6.537481563374545. Old means: -0.161987004486275\n",
      "New prec chol: 6.631282302442071. New means: -0.1674405691472452\n",
      "log prob: 2.45929037554233, change: 0.14156039922809427\n",
      "Old Prec Chol: 6.631282302442071. Old means: -0.1674405691472452\n",
      "New prec chol: 6.648750994180154. New means: -0.16985654740635123\n",
      "log prob: 2.564971648966961, change: 0.10568127342463107\n",
      "Old Prec Chol: 6.648750994180154. Old means: -0.16985654740635123\n",
      "New prec chol: 6.636889575667359. New means: -0.17108486456011165\n",
      "log prob: 2.6561813627597837, change: 0.09120971379282272\n",
      "Old Prec Chol: 6.636889575667359. Old means: -0.17108486456011165\n",
      "New prec chol: 6.648425231643479. New means: -0.1727036463887882\n",
      "log prob: 2.7333848731059414, change: 0.07720351034615769\n",
      "Old Prec Chol: 6.648425231643479. Old means: -0.1727036463887882\n",
      "New prec chol: 6.675251427776331. New means: -0.17600432370559255\n",
      "log prob: 2.79326310096209, change: 0.05987822785614849\n",
      "Old Prec Chol: 6.675251427776331. Old means: -0.17600432370559255\n",
      "New prec chol: 6.7002360642056. New means: -0.17982304209078792\n",
      "log prob: 2.837865989127494, change: 0.04460288816540414\n",
      "Old Prec Chol: 6.7002360642056. Old means: -0.17982304209078792\n",
      "New prec chol: 6.722012623289658. New means: -0.1835376707943166\n",
      "log prob: 2.8717778120023514, change: 0.03391182287485739\n",
      "Old Prec Chol: 6.722012623289658. Old means: -0.1835376707943166\n",
      "New prec chol: 6.726505334949369. New means: -0.18717522358299882\n",
      "log prob: 2.896278476920349, change: 0.02450066491799774\n",
      "Old Prec Chol: 6.726505334949369. Old means: -0.18717522358299882\n",
      "New prec chol: 6.729101891896849. New means: -0.19086594993055175\n",
      "log prob: 2.9154589650479714, change: 0.019180488127622297\n",
      "Old Prec Chol: 6.729101891896849. Old means: -0.19086594993055175\n",
      "New prec chol: 6.719687696704188. New means: -0.1943729664136195\n",
      "log prob: 2.9321336417197763, change: 0.01667467667180489\n",
      "Old Prec Chol: 6.719687696704188. Old means: -0.1943729664136195\n",
      "New prec chol: 6.6861629424980125. New means: -0.19735712820386742\n",
      "log prob: 2.9480121257833867, change: 0.015878484063610365\n",
      "Old Prec Chol: 6.6861629424980125. Old means: -0.19735712820386742\n",
      "New prec chol: 6.654227820164873. New means: -0.2004639633084859\n",
      "log prob: 2.9608539088145767, change: 0.01284178303118999\n",
      "Old Prec Chol: 6.654227820164873. Old means: -0.2004639633084859\n",
      "New prec chol: 6.633572651954161. New means: -0.2041328360996828\n",
      "log prob: 2.972248641485388, change: 0.011394732670811258\n",
      "Old Prec Chol: 6.633572651954161. Old means: -0.2041328360996828\n",
      "New prec chol: 6.619214954341687. New means: -0.20825328868012016\n",
      "log prob: 2.9832836303675268, change: 0.01103498888213883\n",
      "Old Prec Chol: 6.619214954341687. Old means: -0.20825328868012016\n",
      "New prec chol: 6.610355462177425. New means: -0.21271617493620365\n",
      "log prob: 2.993166596692301, change: 0.009882966324774145\n",
      "Converged: True. Number of iterations: 17\n"
     ]
    }
   ],
   "source": [
    "means = gmm_init_params[\"means\"].detach().numpy()\n",
    "weights = gmm_init_params[\"weights\"].detach().numpy()\n",
    "prec_chol = gmm_init_params[\"precision_cholesky\"].detach().numpy()\n",
    "\n",
    "print(f\"Initial prec chol: {prec_chol[IDX][0][0]}. Initial mean: {means[IDX][0]}\")\n",
    "\n",
    "converged = False\n",
    "lower_bound = -np.inf\n",
    "np_log_prob_epochs = []\n",
    "for i in range(EPOCHS):\n",
    "    prev_lower_bound = lower_bound\n",
    "\n",
    "    print(f\"Old Prec Chol: {prec_chol[IDX][0][0]}. Old means: {means[IDX][0]}\")\n",
    "    log_prob, log_resp = _e_step(input_data, means, prec_chol, weights)\n",
    "    prec_chol, weights, means, covar = _m_step(input_data, log_resp)\n",
    "\n",
    "    print(f\"New prec chol: {prec_chol[IDX][0][0]}. New means: {means[IDX][0]}\")\n",
    "\n",
    "    # Converegence\n",
    "    lower_bound = -log_prob\n",
    "    change = abs(lower_bound - prev_lower_bound)\n",
    "    print(f\"log prob: {log_prob}, change: {change}\")\n",
    "    np_log_prob_epochs.append(lower_bound)\n",
    "    if change < CONVERGENCE_TOL:\n",
    "        converged = True\n",
    "        break\n",
    "\n",
    "print(f'Converged: {converged}. Number of iterations: {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch_log_prob_epochs = [log_prob.detach().item() for log_prob in log_prob_epochs]\n",
    "plt.plot(np.arange(0, i+1), torch_log_prob_epochs, label='torch')\n",
    "plt.plot(np.arange(0, i+1), np_log_prob_epochs, label='numpy', linestyle='--')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('log prob')\n",
    "plt.title('500 components')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK Learn GMM Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization 0\n",
      "  Iteration 1\t time lapse 12.56763s\t ll change inf\n",
      "  Iteration 2\t time lapse 6.11678s\t ll change 0.39296\n",
      "  Iteration 3\t time lapse 6.40847s\t ll change 0.21495\n",
      "  Iteration 4\t time lapse 6.80483s\t ll change 0.14734\n",
      "  Iteration 5\t time lapse 6.41462s\t ll change 0.10943\n",
      "  Iteration 6\t time lapse 6.53152s\t ll change 0.09462\n",
      "  Iteration 7\t time lapse 7.16391s\t ll change 0.07798\n",
      "  Iteration 8\t time lapse 6.39741s\t ll change 0.06258\n",
      "  Iteration 9\t time lapse 6.58530s\t ll change 0.04706\n",
      "  Iteration 10\t time lapse 7.47697s\t ll change 0.03431\n",
      "  Iteration 11\t time lapse 7.17728s\t ll change 0.02518\n",
      "  Iteration 12\t time lapse 5.82799s\t ll change 0.02044\n",
      "  Iteration 13\t time lapse 5.33615s\t ll change 0.01895\n",
      "  Iteration 14\t time lapse 5.03128s\t ll change 0.01658\n",
      "  Iteration 15\t time lapse 5.36255s\t ll change 0.01540\n",
      "  Iteration 16\t time lapse 5.06910s\t ll change 0.01316\n",
      "  Iteration 17\t time lapse 5.21010s\t ll change 0.01053\n",
      "  Iteration 18\t time lapse 5.35777s\t ll change 0.00982\n",
      "Initialization converged. time lapse 116.84348s\t lower bound 4.67765.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "init_weights = gmm_init_params[\"weights\"]\n",
    "init_means = gmm_init_params[\"means\"]\n",
    "\n",
    "skgmm = GaussianMixture(n_components=N_COMPONENTS, covariance_type='full', tol=CONVERGENCE_TOL, max_iter=EPOCHS, random_state=0, means_init = init_means, weights_init=init_weights, verbose=2, verbose_interval=1)\n",
    "skgmm.fit(input_data)\n",
    "skgmm_pred = skgmm.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 18)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skgmm.converged_, skgmm.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import LightningDataModule\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_tensor: torch.Tensor):\n",
    "        self.data = data_tensor\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "class CustomDataModule(LightningDataModule):\n",
    "    def __init__(self, data_tensor: torch.Tensor, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.data_tensor = data_tensor\n",
    "        self.batch_size = batch_size\n",
    "    def setup(self, stage=\"\"):\n",
    "        self.custom_ds = CustomDataset(self.data_tensor)\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.custom_ds, batch_size=self.batch_size, shuffle=False, generator=g, worker_init_fn=seed_worker)\n",
    "    \n",
    "custom_dm = CustomDataModule(data_tensor=input_tensor, batch_size=25000)\n",
    "custom_dm.setup(stage=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4973, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4973, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4973, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4973, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4973, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(5):\n",
    "    print(next(iter(custom_dm.train_dataloader()))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                      | Type                    | Params | Mode \n",
      "------------------------------------------------------------------------------\n",
      "0 | gmm_module                | GaussianMixtureModel    | 0      | train\n",
      "1 | vae_module                | FaradayVAE              | 402 K  | eval \n",
      "2 | weight_metric             | WeightsMetric           | 0      | train\n",
      "3 | mean_metric               | MeansMetric             | 0      | train\n",
      "4 | precision_cholesky_metric | PrecisionCholeskyMetric | 0      | train\n",
      "5 | covariance_metric         | CovarianceMetric        | 0      | train\n",
      "6 | nll                       | NLLMetric               | 0      | train\n",
      "------------------------------------------------------------------------------\n",
      "402 K     Trainable params\n",
      "0         Non-trainable params\n",
      "402 K     Total params\n",
      "1.609     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "32        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prec chol: 6.642029285430908. Initial mean: -0.11744648963212967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/charlotte.avery/.virtualenvs/OpenSynth-BNsxhSIM/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:02<00:00,  0.49it/s, v_num=90]Local weights at rank: 0 - means: 0.0055, -0.1369\n",
      "Reduced weights, means, covar: 0.0055, -0.1369, 0.0242\n",
      "log prob:  tensor(-1.7353)\n",
      "Epoch 1: 100%|██████████| 1/1 [00:01<00:00,  0.62it/s, v_num=90]Local weights at rank: 0 - means: 0.0061, -0.1514\n",
      "Reduced weights, means, covar: 0.0061, -0.1514, 0.0242\n",
      "log prob:  tensor(-2.1089)\n",
      "Epoch 2: 100%|██████████| 1/1 [00:01<00:00,  0.62it/s, v_num=90]Local weights at rank: 0 - means: 0.0064, -0.1620\n",
      "Reduced weights, means, covar: 0.0064, -0.1620, 0.0234\n",
      "log prob:  tensor(-2.3179)\n",
      "Epoch 3: 100%|██████████| 1/1 [00:01<00:00,  0.69it/s, v_num=90]Local weights at rank: 0 - means: 0.0066, -0.1674\n",
      "Reduced weights, means, covar: 0.0066, -0.1674, 0.0227\n",
      "log prob:  tensor(-2.4595)\n",
      "Epoch 4: 100%|██████████| 1/1 [00:01<00:00,  0.75it/s, v_num=90]Local weights at rank: 0 - means: 0.0066, -0.1699\n",
      "Reduced weights, means, covar: 0.0066, -0.1699, 0.0226\n",
      "log prob:  tensor(-2.5652)\n",
      "Epoch 5: 100%|██████████| 1/1 [00:01<00:00,  0.81it/s, v_num=90]Local weights at rank: 0 - means: 0.0066, -0.1711\n",
      "Reduced weights, means, covar: 0.0066, -0.1711, 0.0227\n",
      "log prob:  tensor(-2.6564)\n",
      "Epoch 6: 100%|██████████| 1/1 [00:01<00:00,  0.87it/s, v_num=90]Local weights at rank: 0 - means: 0.0065, -0.1727\n",
      "Reduced weights, means, covar: 0.0065, -0.1727, 0.0226\n",
      "log prob:  tensor(-2.7336)\n",
      "Epoch 7: 100%|██████████| 1/1 [00:01<00:00,  0.89it/s, v_num=90]Local weights at rank: 0 - means: 0.0063, -0.1760\n",
      "Reduced weights, means, covar: 0.0063, -0.1760, 0.0224\n",
      "log prob:  tensor(-2.7934)\n",
      "Epoch 8: 100%|██████████| 1/1 [00:01<00:00,  0.76it/s, v_num=90]Local weights at rank: 0 - means: 0.0060, -0.1798\n",
      "Reduced weights, means, covar: 0.0060, -0.1798, 0.0223\n",
      "log prob:  tensor(-2.8381)\n",
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  0.88it/s, v_num=90]Local weights at rank: 0 - means: 0.0059, -0.1835\n",
      "Reduced weights, means, covar: 0.0059, -0.1835, 0.0221\n",
      "log prob:  tensor(-2.8720)\n",
      "Epoch 10: 100%|██████████| 1/1 [00:01<00:00,  0.66it/s, v_num=90]Local weights at rank: 0 - means: 0.0057, -0.1872\n",
      "Reduced weights, means, covar: 0.0057, -0.1872, 0.0221\n",
      "log prob:  tensor(-2.8965)\n",
      "Epoch 11: 100%|██████████| 1/1 [00:01<00:00,  0.62it/s, v_num=90]Local weights at rank: 0 - means: 0.0056, -0.1908\n",
      "Reduced weights, means, covar: 0.0056, -0.1908, 0.0221\n",
      "log prob:  tensor(-2.9157)\n",
      "Epoch 12: 100%|██████████| 1/1 [00:01<00:00,  0.65it/s, v_num=90]Local weights at rank: 0 - means: 0.0055, -0.1943\n",
      "Reduced weights, means, covar: 0.0055, -0.1943, 0.0221\n",
      "log prob:  tensor(-2.9323)\n",
      "Epoch 13: 100%|██████████| 1/1 [00:01<00:00,  0.61it/s, v_num=90]Local weights at rank: 0 - means: 0.0055, -0.1972\n",
      "Reduced weights, means, covar: 0.0055, -0.1972, 0.0224\n",
      "log prob:  tensor(-2.9483)\n",
      "Epoch 14: 100%|██████████| 1/1 [00:01<00:00,  0.55it/s, v_num=90]Local weights at rank: 0 - means: 0.0054, -0.2003\n",
      "Reduced weights, means, covar: 0.0054, -0.2003, 0.0226\n",
      "log prob:  tensor(-2.9611)\n",
      "Epoch 15: 100%|██████████| 1/1 [00:01<00:00,  0.56it/s, v_num=90]Local weights at rank: 0 - means: 0.0053, -0.2039\n",
      "Reduced weights, means, covar: 0.0053, -0.2039, 0.0227\n",
      "log prob:  tensor(-2.9725)\n",
      "Epoch 16: 100%|██████████| 1/1 [00:01<00:00,  0.57it/s, v_num=90]Local weights at rank: 0 - means: 0.0053, -0.2080\n",
      "Reduced weights, means, covar: 0.0053, -0.2080, 0.0228\n",
      "log prob:  tensor(-2.9835)\n",
      "Epoch 17: 100%|██████████| 1/1 [00:01<00:00,  0.55it/s, v_num=90]Local weights at rank: 0 - means: 0.0053, -0.2124\n",
      "Reduced weights, means, covar: 0.0053, -0.2124, 0.0229\n",
      "log prob:  tensor(-2.9934)\n",
      "Epoch 17: 100%|██████████| 1/1 [00:01<00:00,  0.55it/s, v_num=90]\n"
     ]
    }
   ],
   "source": [
    "from opensynth.models.faraday.new_gmm.new_gmm_model import GaussianMixtureLightningModule, GaussianMixtureModel\n",
    "gmm_module = GaussianMixtureModel(\n",
    "    num_components=N_COMPONENTS,\n",
    "    num_features = input_data.shape[1],\n",
    "    reg_covar=REG_COVAR,\n",
    "    print_idx=IDX\n",
    ")\n",
    "gmm_module.initialise(gmm_init_params)\n",
    "print(f\"Initial prec chol: {gmm_module.precision_cholesky[IDX][0][0]}. Initial mean: {gmm_module.means[IDX][0]}\")\n",
    "\n",
    "gmm_lightning_module = GaussianMixtureLightningModule(\n",
    "    gmm_module = gmm_module,\n",
    "    vae_module = vae_model,\n",
    "    num_components = gmm_module.num_components,\n",
    "    num_features = gmm_module.num_features,\n",
    "    reg_covar = gmm_module.reg_covar,\n",
    "    convergence_tolerance = CONVERGENCE_TOL,\n",
    "    sync_on_batch=False\n",
    ")\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, accelerator=\"cpu\", deterministic=True )\n",
    "trainer.fit(gmm_lightning_module, custom_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0053), tensor(-0.2124))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_lightning_module.weight_metric.compute()[0], gmm_lightning_module.mean_metric.compute()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skgmm</th>\n",
       "      <th>numpy</th>\n",
       "      <th>torch</th>\n",
       "      <th>lightning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.177812</td>\n",
       "      <td>-0.212716</td>\n",
       "      <td>-0.212372</td>\n",
       "      <td>-0.212372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.630335</td>\n",
       "      <td>-2.505732</td>\n",
       "      <td>-2.505953</td>\n",
       "      <td>-2.505953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.244407</td>\n",
       "      <td>0.219610</td>\n",
       "      <td>0.218786</td>\n",
       "      <td>0.218786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.060124</td>\n",
       "      <td>0.126913</td>\n",
       "      <td>0.127881</td>\n",
       "      <td>0.127881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.049596</td>\n",
       "      <td>-0.020005</td>\n",
       "      <td>-0.020234</td>\n",
       "      <td>-0.020234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.029421</td>\n",
       "      <td>-0.061361</td>\n",
       "      <td>-0.061332</td>\n",
       "      <td>-0.061332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.387513</td>\n",
       "      <td>0.331236</td>\n",
       "      <td>0.330309</td>\n",
       "      <td>0.330309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.323282</td>\n",
       "      <td>2.380669</td>\n",
       "      <td>2.381529</td>\n",
       "      <td>2.381529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.012398</td>\n",
       "      <td>-0.012253</td>\n",
       "      <td>-0.012595</td>\n",
       "      <td>-0.012595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-2.338854</td>\n",
       "      <td>-2.501915</td>\n",
       "      <td>-2.501398</td>\n",
       "      <td>-2.501398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.449570</td>\n",
       "      <td>0.347859</td>\n",
       "      <td>0.347187</td>\n",
       "      <td>0.347187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.675564</td>\n",
       "      <td>0.796676</td>\n",
       "      <td>0.795087</td>\n",
       "      <td>0.795087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.061282</td>\n",
       "      <td>-0.141880</td>\n",
       "      <td>-0.142444</td>\n",
       "      <td>-0.142444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.810963</td>\n",
       "      <td>0.740431</td>\n",
       "      <td>0.740757</td>\n",
       "      <td>0.740757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.408608</td>\n",
       "      <td>1.468580</td>\n",
       "      <td>1.467806</td>\n",
       "      <td>1.467806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-2.258430</td>\n",
       "      <td>-2.242791</td>\n",
       "      <td>-2.242586</td>\n",
       "      <td>-2.242586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.677418</td>\n",
       "      <td>4.710169</td>\n",
       "      <td>4.711856</td>\n",
       "      <td>4.711856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.403793</td>\n",
       "      <td>0.425830</td>\n",
       "      <td>0.425249</td>\n",
       "      <td>0.425249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       skgmm     numpy     torch  lightning\n",
       "0  -0.177812 -0.212716 -0.212372  -0.212372\n",
       "1  -2.630335 -2.505732 -2.505953  -2.505953\n",
       "2   0.244407  0.219610  0.218786   0.218786\n",
       "3   0.060124  0.126913  0.127881   0.127881\n",
       "4  -0.049596 -0.020005 -0.020234  -0.020234\n",
       "5  -0.029421 -0.061361 -0.061332  -0.061332\n",
       "6   0.387513  0.331236  0.330309   0.330309\n",
       "7   2.323282  2.380669  2.381529   2.381529\n",
       "8  -0.012398 -0.012253 -0.012595  -0.012595\n",
       "9  -2.338854 -2.501915 -2.501398  -2.501398\n",
       "10  0.449570  0.347859  0.347187   0.347187\n",
       "11  0.675564  0.796676  0.795087   0.795087\n",
       "12 -0.061282 -0.141880 -0.142444  -0.142444\n",
       "13  0.810963  0.740431  0.740757   0.740757\n",
       "14  1.408608  1.468580  1.467806   1.467806\n",
       "15 -2.258430 -2.242791 -2.242586  -2.242586\n",
       "16  4.677418  4.710169  4.711856   4.711856\n",
       "17  0.403793  0.425830  0.425249   0.425249"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare_means = pd.DataFrame()\n",
    "df_compare_means[\"skgmm\"] = skgmm.means_[IDX]\n",
    "df_compare_means[\"numpy\"] = means[IDX]\n",
    "df_compare_means[\"torch\"] = trained_model.means[IDX]\n",
    "df_compare_means[\"lightning\"] = gmm_lightning_module.gmm_module.means[IDX]\n",
    "df_compare_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1745e-01, -2.6930e+00,  1.3211e-01,  2.0796e-01,  8.0334e-02,\n",
       "        -4.6100e-03,  2.8707e-01,  2.2114e+00, -2.9962e-02, -2.3726e+00,\n",
       "         3.7961e-01,  4.0148e-01, -4.9165e-03,  7.3653e-01,  1.4565e+00,\n",
       "        -2.3367e+00,  4.8403e+00,  5.3782e-01])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_init_params[\"means\"][IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skgmm</th>\n",
       "      <th>numpy</th>\n",
       "      <th>torch</th>\n",
       "      <th>lightning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022647</td>\n",
       "      <td>0.022885</td>\n",
       "      <td>0.022873</td>\n",
       "      <td>0.022873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.066062</td>\n",
       "      <td>-0.070538</td>\n",
       "      <td>-0.070572</td>\n",
       "      <td>-0.070572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.031553</td>\n",
       "      <td>-0.037055</td>\n",
       "      <td>-0.036989</td>\n",
       "      <td>-0.036989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034193</td>\n",
       "      <td>0.040163</td>\n",
       "      <td>0.040070</td>\n",
       "      <td>0.040070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005880</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>0.002553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.007099</td>\n",
       "      <td>0.007122</td>\n",
       "      <td>0.007122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.025484</td>\n",
       "      <td>-0.032502</td>\n",
       "      <td>-0.032373</td>\n",
       "      <td>-0.032373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.024867</td>\n",
       "      <td>0.038144</td>\n",
       "      <td>0.037946</td>\n",
       "      <td>0.037946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.008028</td>\n",
       "      <td>-0.010404</td>\n",
       "      <td>-0.010352</td>\n",
       "      <td>-0.010352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001745</td>\n",
       "      <td>-0.009538</td>\n",
       "      <td>-0.009610</td>\n",
       "      <td>-0.009610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.004443</td>\n",
       "      <td>-0.011328</td>\n",
       "      <td>-0.011214</td>\n",
       "      <td>-0.011214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.078811</td>\n",
       "      <td>-0.074499</td>\n",
       "      <td>-0.074424</td>\n",
       "      <td>-0.074424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.004723</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.002088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.024915</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>0.025389</td>\n",
       "      <td>0.025389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.008354</td>\n",
       "      <td>-0.008811</td>\n",
       "      <td>-0.008645</td>\n",
       "      <td>-0.008645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.029624</td>\n",
       "      <td>-0.034760</td>\n",
       "      <td>-0.034843</td>\n",
       "      <td>-0.034843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.023554</td>\n",
       "      <td>-0.010180</td>\n",
       "      <td>-0.010315</td>\n",
       "      <td>-0.010315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.003680</td>\n",
       "      <td>-0.000780</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>-0.000671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       skgmm     numpy     torch  lightning\n",
       "0   0.022647  0.022885  0.022873   0.022873\n",
       "1  -0.066062 -0.070538 -0.070572  -0.070572\n",
       "2  -0.031553 -0.037055 -0.036989  -0.036989\n",
       "3   0.034193  0.040163  0.040070   0.040070\n",
       "4   0.005880  0.002422  0.002553   0.002553\n",
       "5   0.008600  0.007099  0.007122   0.007122\n",
       "6  -0.025484 -0.032502 -0.032373  -0.032373\n",
       "7   0.024867  0.038144  0.037946   0.037946\n",
       "8  -0.008028 -0.010404 -0.010352  -0.010352\n",
       "9   0.001745 -0.009538 -0.009610  -0.009610\n",
       "10 -0.004443 -0.011328 -0.011214  -0.011214\n",
       "11 -0.078811 -0.074499 -0.074424  -0.074424\n",
       "12  0.004723  0.001993  0.002088   0.002088\n",
       "13  0.024915  0.025441  0.025389   0.025389\n",
       "14 -0.008354 -0.008811 -0.008645  -0.008645\n",
       "15 -0.029624 -0.034760 -0.034843  -0.034843\n",
       "16 -0.023554 -0.010180 -0.010315  -0.010315\n",
       "17 -0.003680 -0.000780 -0.000671  -0.000671"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare_covar = pd.DataFrame()\n",
    "df_compare_covar[\"skgmm\"] = skgmm.covariances_[IDX][0]\n",
    "df_compare_covar[\"numpy\"] = covar[IDX][0]\n",
    "df_compare_covar[\"torch\"] = trained_model.covariances.detach().numpy()[IDX][0]\n",
    "df_compare_covar[\"lightning\"] = gmm_lightning_module.gmm_module.covariances.detach().numpy()[IDX][0]\n",
    "df_compare_covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skgmm</th>\n",
       "      <th>numpy</th>\n",
       "      <th>torch</th>\n",
       "      <th>lightning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.645059</td>\n",
       "      <td>6.610355</td>\n",
       "      <td>6.612116</td>\n",
       "      <td>6.612116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.610498</td>\n",
       "      <td>4.368615</td>\n",
       "      <td>4.367012</td>\n",
       "      <td>4.367012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.306633</td>\n",
       "      <td>6.294993</td>\n",
       "      <td>6.288661</td>\n",
       "      <td>6.288661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.631353</td>\n",
       "      <td>-0.398878</td>\n",
       "      <td>-0.395308</td>\n",
       "      <td>-0.395308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.371952</td>\n",
       "      <td>-3.881848</td>\n",
       "      <td>-3.935514</td>\n",
       "      <td>-3.935514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-14.655278</td>\n",
       "      <td>-12.413238</td>\n",
       "      <td>-12.399793</td>\n",
       "      <td>-12.399793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.701320</td>\n",
       "      <td>9.999307</td>\n",
       "      <td>10.014248</td>\n",
       "      <td>10.014248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.580544</td>\n",
       "      <td>6.946692</td>\n",
       "      <td>6.951899</td>\n",
       "      <td>6.951899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.601643</td>\n",
       "      <td>3.532776</td>\n",
       "      <td>3.516963</td>\n",
       "      <td>3.516963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-7.043057</td>\n",
       "      <td>-6.561041</td>\n",
       "      <td>-6.538032</td>\n",
       "      <td>-6.538032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.561974</td>\n",
       "      <td>4.005077</td>\n",
       "      <td>4.005848</td>\n",
       "      <td>4.005848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.829812</td>\n",
       "      <td>5.304793</td>\n",
       "      <td>5.319723</td>\n",
       "      <td>5.319723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-9.063113</td>\n",
       "      <td>-7.404270</td>\n",
       "      <td>-7.390814</td>\n",
       "      <td>-7.390814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-3.261489</td>\n",
       "      <td>-1.128210</td>\n",
       "      <td>-1.148718</td>\n",
       "      <td>-1.148718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.297851</td>\n",
       "      <td>0.733817</td>\n",
       "      <td>0.713779</td>\n",
       "      <td>0.713779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-3.590018</td>\n",
       "      <td>-2.616844</td>\n",
       "      <td>-2.619453</td>\n",
       "      <td>-2.619453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.422232</td>\n",
       "      <td>2.742836</td>\n",
       "      <td>2.744523</td>\n",
       "      <td>2.744523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.030066</td>\n",
       "      <td>-5.327221</td>\n",
       "      <td>-5.333878</td>\n",
       "      <td>-5.333878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        skgmm      numpy      torch  lightning\n",
       "0    6.645059   6.610355   6.612116   6.612116\n",
       "1    4.610498   4.368615   4.367012   4.367012\n",
       "2    5.306633   6.294993   6.288661   6.288661\n",
       "3   -0.631353  -0.398878  -0.395308  -0.395308\n",
       "4   -3.371952  -3.881848  -3.935514  -3.935514\n",
       "5  -14.655278 -12.413238 -12.399793 -12.399793\n",
       "6   10.701320   9.999307  10.014248  10.014248\n",
       "7    8.580544   6.946692   6.951899   6.951899\n",
       "8    1.601643   3.532776   3.516963   3.516963\n",
       "9   -7.043057  -6.561041  -6.538032  -6.538032\n",
       "10   1.561974   4.005077   4.005848   4.005848\n",
       "11   5.829812   5.304793   5.319723   5.319723\n",
       "12  -9.063113  -7.404270  -7.390814  -7.390814\n",
       "13  -3.261489  -1.128210  -1.148718  -1.148718\n",
       "14   5.297851   0.733817   0.713779   0.713779\n",
       "15  -3.590018  -2.616844  -2.619453  -2.619453\n",
       "16   6.422232   2.742836   2.744523   2.744523\n",
       "17   1.030066  -5.327221  -5.333878  -5.333878"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare_pre_chol = pd.DataFrame()\n",
    "df_compare_pre_chol[\"skgmm\"] = skgmm.precisions_cholesky_[IDX][0]\n",
    "df_compare_pre_chol[\"numpy\"] = prec_chol[IDX][0]\n",
    "df_compare_pre_chol[\"torch\"] = trained_model.precision_cholesky.detach().numpy()[IDX][0]\n",
    "df_compare_pre_chol[\"lightning\"] = gmm_lightning_module.gmm_module.precision_cholesky.detach().numpy()[IDX][0]\n",
    "df_compare_pre_chol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skgmm</th>\n",
       "      <th>numpy</th>\n",
       "      <th>torch</th>\n",
       "      <th>lightning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>0.005259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002965</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.002924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.007004</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.005051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      skgmm     numpy     torch  lightning\n",
       "0  0.004938  0.005264  0.005259   0.005259\n",
       "1  0.000120  0.000120  0.000120   0.000120\n",
       "2  0.002965  0.002925  0.002924   0.002924\n",
       "3  0.001440  0.001375  0.001375   0.001375\n",
       "4  0.001119  0.001119  0.001119   0.001119\n",
       "5  0.000200  0.000200  0.000200   0.000200\n",
       "6  0.007004  0.005061  0.005051   0.005051\n",
       "7  0.000040  0.000040  0.000040   0.000040\n",
       "8  0.001442  0.001443  0.001443   0.001443\n",
       "9  0.000080  0.000080  0.000080   0.000080"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare_weights = pd.DataFrame()\n",
    "df_compare_weights[\"skgmm\"] = skgmm.weights_[:10]\n",
    "df_compare_weights[\"numpy\"] = weights[:10]\n",
    "df_compare_weights[\"torch\"] = trained_model.weights[:10]\n",
    "df_compare_weights[\"lightning\"] = gmm_lightning_module.gmm_module.weights.detach().numpy()[:10]\n",
    "df_compare_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(means_, covariances_, weights_, n_samples):\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    n_samples_comp = rng.multinomial(n_samples, weights_)\n",
    "    \n",
    "    X = np.vstack(\n",
    "            [\n",
    "                rng.multivariate_normal(mean, covariance, int(sample))\n",
    "                for (mean, covariance, sample) in zip(\n",
    "                    means_, covariances_, n_samples_comp\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    y = np.concatenate(\n",
    "        [np.full(sample, j, dtype=int) for j, sample in enumerate(n_samples_comp)]\n",
    "    )\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_sample(means_, covariances_, weights_, n_samples):\n",
    "    # Set up the random generator with a specified seed\n",
    "    generator = torch.Generator().manual_seed(RANDOM_STATE)\n",
    "    \n",
    "    # Sample component counts from the multinomial distribution\n",
    "    n_samples_comp = torch.multinomial(weights_, n_samples, replacement=True, generator=generator).bincount(minlength=len(weights_))\n",
    "    \n",
    "    # Initialize lists to collect samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Sample from each component based on the number of samples\n",
    "    for j, (mean, covariance, sample_count) in enumerate(zip(means_, covariances_, n_samples_comp)):\n",
    "        if sample_count > 0:  # Only sample if we need samples from this component\n",
    "            dist = torch.distributions.MultivariateNormal(\n",
    "                mean, covariance\n",
    "            )\n",
    "            samples = dist.sample((sample_count,))\n",
    "            X.append(samples)\n",
    "            y.append(torch.full((sample_count,), j, dtype=torch.int64))\n",
    "    \n",
    "    # Concatenate all samples and labels into single tensors\n",
    "    X = torch.vstack(X)\n",
    "    y = torch.cat(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.12875661e-01, -3.12396643e+00,  4.06773316e-04,  2.82460393e-01,\n",
       "         1.16172226e-01, -8.79575001e-02,  2.30796585e-01,  2.46989285e+00,\n",
       "        -8.22130688e-02, -2.84766877e+00,  3.78023198e-01,  5.54822997e-01,\n",
       "         9.32978802e-02,  8.86258616e-01,  1.68589723e+00, -2.69251050e+00,\n",
       "         3.42595060e+00,  6.72179315e-01]),\n",
       " 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skgmm_samples = sample(skgmm.means_, skgmm.covariances_, skgmm.weights_, n_samples = N_SAMPLES)\n",
    "\n",
    "skgmm_X, skgmm_y = skgmm_samples\n",
    "skgmm_X[IDX], skgmm_y[IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.1912099 , -2.71073442,  0.79864298, -0.44077533,  0.11529497,\n",
       "         0.00573171,  0.92426352,  2.063336  ,  0.12719504, -2.50545085,\n",
       "         0.88694172,  1.28716994,  0.23383632,  0.83769176,  1.87811581,\n",
       "        -2.51606552,  4.87421859,  0.22942809]),\n",
       " 0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = sample(means, covar, weights, n_samples = N_SAMPLES)\n",
    "\n",
    "X, y = samples\n",
    "X[IDX], y[IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.3360e-01, -2.7357e+00,  5.0481e-01, -3.0986e-01, -4.0325e-01,\n",
       "          5.5966e-02,  5.2489e-01,  1.7848e+00,  2.3873e-03, -1.9278e+00,\n",
       "          6.8150e-01,  8.2515e-01,  6.2167e-01,  6.8055e-01,  1.0813e+00,\n",
       "         -2.2938e+00,  6.7474e+00,  1.9539e-01]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_samples = torch_sample(trained_model.means, trained_model.covariances, trained_model.weights, n_samples = N_SAMPLES)\n",
    "train_model_X, train_model_y = train_model_samples\n",
    "train_model_X[IDX], train_model_y[IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.3360e-01, -2.7357e+00,  5.0481e-01, -3.0986e-01, -4.0325e-01,\n",
       "          5.5966e-02,  5.2489e-01,  1.7848e+00,  2.3873e-03, -1.9278e+00,\n",
       "          6.8150e-01,  8.2515e-01,  6.2167e-01,  6.8055e-01,  1.0813e+00,\n",
       "         -2.2938e+00,  6.7474e+00,  1.9539e-01]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_lightning_samples = torch_sample(gmm_lightning_module.gmm_module.means, gmm_lightning_module.gmm_module.covariances, gmm_lightning_module.gmm_module.weights, n_samples = N_SAMPLES)\n",
    "gmm_lightning_X, gmm_lightning_y = train_model_samples\n",
    "gmm_lightning_X[IDX], gmm_lightning_y[IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenSynth-BNsxhSIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
